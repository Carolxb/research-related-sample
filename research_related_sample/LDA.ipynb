{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7864da1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.1.2-cp38-cp38-macosx_10_9_x86_64.whl (24.0 MB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 24.0 MB 960 kB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17.0 in /Users/carol/opt/anaconda3/lib/python3.8/site-packages (from gensim) (1.20.3)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /Users/carol/opt/anaconda3/lib/python3.8/site-packages (from gensim) (1.7.1)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 58 kB 8.5 MB/s  eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: smart-open, gensim\n",
      "Successfully installed gensim-4.1.2 smart-open-5.2.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "794d0ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /Users/carol/opt/anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - spacy\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    catalogue-2.0.6            |   py38h50d1736_1          31 KB  conda-forge\n",
      "    conda-4.11.0               |   py38h50d1736_0        16.9 MB  conda-forge\n",
      "    cymem-2.0.6                |   py38ha048514_2          33 KB  conda-forge\n",
      "    cython-blis-0.7.5          |   py38hbe852b5_1         5.1 MB  conda-forge\n",
      "    langcodes-3.3.0            |     pyhd8ed1ab_0         156 KB  conda-forge\n",
      "    murmurhash-1.0.6           |   py38ha048514_2          22 KB  conda-forge\n",
      "    pathy-0.6.1                |     pyhd8ed1ab_0          37 KB  conda-forge\n",
      "    preshed-3.0.6              |   py38ha048514_1          87 KB  conda-forge\n",
      "    pydantic-1.8.2             |   py38h96a0964_2         2.1 MB  conda-forge\n",
      "    shellingham-1.4.0          |     pyh44b312d_0          11 KB  conda-forge\n",
      "    smart_open-5.2.1           |     pyhd8ed1ab_0          43 KB  conda-forge\n",
      "    spacy-3.2.2                |   py38hb0f0857_0         5.4 MB  conda-forge\n",
      "    spacy-legacy-3.0.8         |     pyhd8ed1ab_0          15 KB  conda-forge\n",
      "    spacy-loggers-1.0.1        |     pyhd8ed1ab_0          10 KB  conda-forge\n",
      "    srsly-2.4.2                |   py38ha048514_1         506 KB  conda-forge\n",
      "    thinc-8.0.13               |   py38hb0f0857_0         668 KB  conda-forge\n",
      "    typer-0.4.0                |     pyhd8ed1ab_0          26 KB  conda-forge\n",
      "    typing-extensions-3.10.0.2 |       hd3eb1b0_0          12 KB\n",
      "    wasabi-0.9.0               |     pyhd8ed1ab_0          24 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        31.1 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  catalogue          conda-forge/osx-64::catalogue-2.0.6-py38h50d1736_1\n",
      "  cymem              conda-forge/osx-64::cymem-2.0.6-py38ha048514_2\n",
      "  cython-blis        conda-forge/osx-64::cython-blis-0.7.5-py38hbe852b5_1\n",
      "  langcodes          conda-forge/noarch::langcodes-3.3.0-pyhd8ed1ab_0\n",
      "  murmurhash         conda-forge/osx-64::murmurhash-1.0.6-py38ha048514_2\n",
      "  pathy              conda-forge/noarch::pathy-0.6.1-pyhd8ed1ab_0\n",
      "  preshed            conda-forge/osx-64::preshed-3.0.6-py38ha048514_1\n",
      "  pydantic           conda-forge/osx-64::pydantic-1.8.2-py38h96a0964_2\n",
      "  shellingham        conda-forge/noarch::shellingham-1.4.0-pyh44b312d_0\n",
      "  smart_open         conda-forge/noarch::smart_open-5.2.1-pyhd8ed1ab_0\n",
      "  spacy              conda-forge/osx-64::spacy-3.2.2-py38hb0f0857_0\n",
      "  spacy-legacy       conda-forge/noarch::spacy-legacy-3.0.8-pyhd8ed1ab_0\n",
      "  spacy-loggers      conda-forge/noarch::spacy-loggers-1.0.1-pyhd8ed1ab_0\n",
      "  srsly              conda-forge/osx-64::srsly-2.4.2-py38ha048514_1\n",
      "  thinc              conda-forge/osx-64::thinc-8.0.13-py38hb0f0857_0\n",
      "  typer              conda-forge/noarch::typer-0.4.0-pyhd8ed1ab_0\n",
      "  typing-extensions  pkgs/main/noarch::typing-extensions-3.10.0.2-hd3eb1b0_0\n",
      "  wasabi             conda-forge/noarch::wasabi-0.9.0-pyhd8ed1ab_0\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  certifi            pkgs/main::certifi-2021.10.8-py38hecd~ --> conda-forge::certifi-2021.10.8-py38h50d1736_1\n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  ca-certificates    pkgs/main::ca-certificates-2021.10.26~ --> conda-forge::ca-certificates-2021.10.8-h033912b_0\n",
      "  conda              pkgs/main::conda-4.11.0-py38hecd8cb5_0 --> conda-forge::conda-4.11.0-py38h50d1736_0\n",
      "  openssl              pkgs/main::openssl-1.1.1l-h9ed2024_0 --> conda-forge::openssl-1.1.1l-h0d85af4_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "pydantic-1.8.2       | 2.1 MB    | ##################################### | 100% \n",
      "wasabi-0.9.0         | 24 KB     | ##################################### | 100% \n",
      "srsly-2.4.2          | 506 KB    | ##################################### | 100% \n",
      "shellingham-1.4.0    | 11 KB     | ##################################### | 100% \n",
      "typing-extensions-3. | 12 KB     | ##################################### | 100% \n",
      "catalogue-2.0.6      | 31 KB     | ##################################### | 100% \n",
      "thinc-8.0.13         | 668 KB    | ##################################### | 100% \n",
      "conda-4.11.0         | 16.9 MB   | ##################################### | 100% \n",
      "cymem-2.0.6          | 33 KB     | ##################################### | 100% \n",
      "spacy-loggers-1.0.1  | 10 KB     | ##################################### | 100% \n",
      "murmurhash-1.0.6     | 22 KB     | ##################################### | 100% \n",
      "smart_open-5.2.1     | 43 KB     | ##################################### | 100% \n",
      "preshed-3.0.6        | 87 KB     | ##################################### | 100% \n",
      "spacy-legacy-3.0.8   | 15 KB     | ##################################### | 100% \n",
      "pathy-0.6.1          | 37 KB     | ##################################### | 100% \n",
      "spacy-3.2.2          | 5.4 MB    | ##################################### | 100% \n",
      "langcodes-3.3.0      | 156 KB    | ##################################### | 100% \n",
      "cython-blis-0.7.5    | 5.1 MB    | ##################################### | 100% \n",
      "typer-0.4.0          | 26 KB     | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install -c conda-forge spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fade845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /Users/carol/opt/anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - pyldavis\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    funcy-1.17                 |     pyhd8ed1ab_0          30 KB  conda-forge\n",
      "    pyldavis-3.3.1             |     pyhd8ed1ab_0         114 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         144 KB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  funcy              conda-forge/noarch::funcy-1.17-pyhd8ed1ab_0\n",
      "  pyldavis           conda-forge/noarch::pyldavis-3.3.1-pyhd8ed1ab_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "funcy-1.17           | 30 KB     | ##################################### | 100% \n",
      "pyldavis-3.3.1       | 114 KB    | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install -c conda-forge pyldavis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4a77213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n",
      "Requirement already satisfied: spacy<3.3.0,>=3.2.0 in /Users/carol/opt/anaconda3/lib/python3.8/site-packages (from en-core-web-sm==3.2.0) (3.2.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/carol/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/carol/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /Users/carol/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.8)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/carol/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.6)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/carol/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.20.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/carol/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.62.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/carol/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.6)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /Users/carol/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/carol/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.26.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /Users/carol/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.9.0)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /Users/carol/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.0)\n",
      "Requirement already satisfied: jinja2 in /Users/carol/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/carol/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Users/carol/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.6.1)\n",
      "Requirement already satisfied: setuptools in /Users/carol/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (58.0.4)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /Users/carol/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/carol/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (21.3)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /Users/carol/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.13)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/carol/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.1)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /Users/carol/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.7.5)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/carol/opt/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /Users/carol/opt/anaconda3/lib/python3.8/site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/carol/opt/anaconda3/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.10.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/carol/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/carol/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/carol/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/carol/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.26.7)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/carol/opt/anaconda3/lib/python3.8/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/carol/opt/anaconda3/lib/python3.8/site-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.2.0\n",
      "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.cli.download import download\n",
    "download(model=\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3d5a3592",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #import pandas\n",
    "reviews_df=pd.read_csv('sentiment_result.csv') #read data from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "67573224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>crawl_date_x</th>\n",
       "      <th>user_id</th>\n",
       "      <th>username</th>\n",
       "      <th>date</th>\n",
       "      <th>caption</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>follower_count</th>\n",
       "      <th>like_count</th>\n",
       "      <th>...</th>\n",
       "      <th>Old</th>\n",
       "      <th>Product</th>\n",
       "      <th>Sign</th>\n",
       "      <th>Text</th>\n",
       "      <th>Cartoon</th>\n",
       "      <th>statistics</th>\n",
       "      <th>negativity</th>\n",
       "      <th>neutrality</th>\n",
       "      <th>positivity</th>\n",
       "      <th>compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.644339e+09</td>\n",
       "      <td>2.960596e+08</td>\n",
       "      <td>usernamesuckidge</td>\n",
       "      <td>1.563923e+09</td>\n",
       "      <td>#signofthetimes #welcometo2019 #nosmoking #nov...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.644339e+09</td>\n",
       "      <td>2.320914e+09</td>\n",
       "      <td>holisticatedme</td>\n",
       "      <td>1.465754e+09</td>\n",
       "      <td>Whenever I see someone smoking I say you know ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>706.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.829</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.3535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.644339e+09</td>\n",
       "      <td>2.246866e+09</td>\n",
       "      <td>stormcrowalehouse</td>\n",
       "      <td>1.461802e+09</td>\n",
       "      <td>Just in case you thought you could get away wi...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3631.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.915</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.3818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1.644339e+09</td>\n",
       "      <td>4.325633e+09</td>\n",
       "      <td>laurencallenderz</td>\n",
       "      <td>1.570685e+09</td>\n",
       "      <td>üÖ•üÖêüÖüüÖî üÖòüÖ¢ üÖùüÖûüÖ£ üÖíüÖûüÖûüÖõ üÖ¢üÖû üÖìüÖûüÖùüÖ£ üÖôüÖ§üÖ§üÖõ I use to vape be...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>194.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.773</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.2960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1.644339e+09</td>\n",
       "      <td>9.235050e+09</td>\n",
       "      <td>sphsvapes</td>\n",
       "      <td>1.542394e+09</td>\n",
       "      <td>Asking the big questions #stopvaping</td>\n",
       "      <td>3.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8967</th>\n",
       "      <td>8967</td>\n",
       "      <td>8967</td>\n",
       "      <td>1.644339e+09</td>\n",
       "      <td>2.347172e+10</td>\n",
       "      <td>southyarracounselling</td>\n",
       "      <td>1.573712e+09</td>\n",
       "      <td>Now that vaping is being banned by so many, pe...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.742</td>\n",
       "      <td>0.127</td>\n",
       "      <td>-0.0258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8968</th>\n",
       "      <td>8968</td>\n",
       "      <td>8968</td>\n",
       "      <td>1.644339e+09</td>\n",
       "      <td>8.090645e+09</td>\n",
       "      <td>skylartatu</td>\n",
       "      <td>1.550698e+09</td>\n",
       "      <td>#WednesdayWisdom\\nStop trying to be perfect an...</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1089.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.819</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.9314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8969</th>\n",
       "      <td>8969</td>\n",
       "      <td>8969</td>\n",
       "      <td>1.644339e+09</td>\n",
       "      <td>2.205220e+09</td>\n",
       "      <td>allencarrseasyway</td>\n",
       "      <td>1.574772e+09</td>\n",
       "      <td>John Dicey, Global CEO of Allen Carr‚Äôs Easyway...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1433.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.821</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.3182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8970</th>\n",
       "      <td>8970</td>\n",
       "      <td>8970</td>\n",
       "      <td>1.644339e+09</td>\n",
       "      <td>4.779968e+08</td>\n",
       "      <td>doughboy0043</td>\n",
       "      <td>1.441838e+09</td>\n",
       "      <td>1st mile 9:35??? What??? That has gotta be a P...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.863</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.6958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8971</th>\n",
       "      <td>8971</td>\n",
       "      <td>8971</td>\n",
       "      <td>1.644339e+09</td>\n",
       "      <td>3.038993e+09</td>\n",
       "      <td>maxwell_di_paolo_photography</td>\n",
       "      <td>1.458941e+09</td>\n",
       "      <td>Candid of Steve and Sam #photo ‚ù§Ô∏è #friends #bw...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>310.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8972 rows √ó 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  Unnamed: 0.1  crawl_date_x       user_id  \\\n",
       "0              0             0  1.644339e+09  2.960596e+08   \n",
       "1              1             1  1.644339e+09  2.320914e+09   \n",
       "2              2             2  1.644339e+09  2.246866e+09   \n",
       "3              3             3  1.644339e+09  4.325633e+09   \n",
       "4              4             4  1.644339e+09  9.235050e+09   \n",
       "...          ...           ...           ...           ...   \n",
       "8967        8967          8967  1.644339e+09  2.347172e+10   \n",
       "8968        8968          8968  1.644339e+09  8.090645e+09   \n",
       "8969        8969          8969  1.644339e+09  2.205220e+09   \n",
       "8970        8970          8970  1.644339e+09  4.779968e+08   \n",
       "8971        8971          8971  1.644339e+09  3.038993e+09   \n",
       "\n",
       "                          username          date  \\\n",
       "0                 usernamesuckidge  1.563923e+09   \n",
       "1                   holisticatedme  1.465754e+09   \n",
       "2                stormcrowalehouse  1.461802e+09   \n",
       "3                 laurencallenderz  1.570685e+09   \n",
       "4                        sphsvapes  1.542394e+09   \n",
       "...                            ...           ...   \n",
       "8967         southyarracounselling  1.573712e+09   \n",
       "8968                    skylartatu  1.550698e+09   \n",
       "8969             allencarrseasyway  1.574772e+09   \n",
       "8970                  doughboy0043  1.441838e+09   \n",
       "8971  maxwell_di_paolo_photography  1.458941e+09   \n",
       "\n",
       "                                                caption  comment_count  \\\n",
       "0     #signofthetimes #welcometo2019 #nosmoking #nov...            0.0   \n",
       "1     Whenever I see someone smoking I say you know ...            3.0   \n",
       "2     Just in case you thought you could get away wi...            1.0   \n",
       "3     üÖ•üÖêüÖüüÖî üÖòüÖ¢ üÖùüÖûüÖ£ üÖíüÖûüÖûüÖõ üÖ¢üÖû üÖìüÖûüÖùüÖ£ üÖôüÖ§üÖ§üÖõ I use to vape be...            7.0   \n",
       "4                  Asking the big questions #stopvaping            3.0   \n",
       "...                                                 ...            ...   \n",
       "8967  Now that vaping is being banned by so many, pe...            1.0   \n",
       "8968  #WednesdayWisdom\\nStop trying to be perfect an...           16.0   \n",
       "8969  John Dicey, Global CEO of Allen Carr‚Äôs Easyway...            1.0   \n",
       "8970  1st mile 9:35??? What??? That has gotta be a P...            4.0   \n",
       "8971  Candid of Steve and Sam #photo ‚ù§Ô∏è #friends #bw...            1.0   \n",
       "\n",
       "      follower_count  like_count  ...  Old Product Sign  Text  Cartoon  \\\n",
       "0              280.0        16.0  ...    0       0    0     0        0   \n",
       "1              706.0        25.0  ...    0       0    0     1        0   \n",
       "2             3631.0        46.0  ...    0       0    0     1        0   \n",
       "3              194.0        23.0  ...    0       0    0     0        0   \n",
       "4              236.0        58.0  ...    0       0    0     0        0   \n",
       "...              ...         ...  ...  ...     ...  ...   ...      ...   \n",
       "8967            25.0         5.0  ...    0       0    0     0        0   \n",
       "8968          1089.0        47.0  ...    0       0    0     1        0   \n",
       "8969          1433.0        13.0  ...    0       0    0     1        0   \n",
       "8970           120.0        27.0  ...    0       0    0     1        0   \n",
       "8971           310.0        21.0  ...    0       1    0     0        0   \n",
       "\n",
       "     statistics  negativity  neutrality  positivity  compound  \n",
       "0             0       0.000       1.000       0.000    0.0000  \n",
       "1             0       0.072       0.829       0.099    0.3535  \n",
       "2             0       0.000       0.915       0.085    0.3818  \n",
       "3             0       0.112       0.773       0.115    0.2960  \n",
       "4             0       0.000       1.000       0.000    0.0000  \n",
       "...         ...         ...         ...         ...       ...  \n",
       "8967          0       0.131       0.742       0.127   -0.0258  \n",
       "8968          0       0.052       0.819       0.129    0.9314  \n",
       "8969          0       0.045       0.821       0.133    0.3182  \n",
       "8970          0       0.137       0.863       0.000   -0.6958  \n",
       "8971          0       0.000       1.000       0.000    0.0000  \n",
       "\n",
       "[8972 rows x 37 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b9f64530",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1e573f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df = reviews_df.dropna(subset=['crawl_date_x', 'user_id', 'username','date','caption','comment_count','follower_count','like_count','media_count','pic_url','hashtags','hashtag_count'], how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6c1396d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>crawl_date_x</th>\n",
       "      <th>user_id</th>\n",
       "      <th>username</th>\n",
       "      <th>date</th>\n",
       "      <th>caption</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>follower_count</th>\n",
       "      <th>like_count</th>\n",
       "      <th>...</th>\n",
       "      <th>Old</th>\n",
       "      <th>Product</th>\n",
       "      <th>Sign</th>\n",
       "      <th>Text</th>\n",
       "      <th>Cartoon</th>\n",
       "      <th>statistics</th>\n",
       "      <th>negativity</th>\n",
       "      <th>neutrality</th>\n",
       "      <th>positivity</th>\n",
       "      <th>compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.644339e+09</td>\n",
       "      <td>2.960596e+08</td>\n",
       "      <td>usernamesuckidge</td>\n",
       "      <td>1.563923e+09</td>\n",
       "      <td>#signofthetimes #welcometo2019 #nosmoking #nov...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.644339e+09</td>\n",
       "      <td>2.320914e+09</td>\n",
       "      <td>holisticatedme</td>\n",
       "      <td>1.465754e+09</td>\n",
       "      <td>Whenever I see someone smoking I say you know ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>706.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.829</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.3535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.644339e+09</td>\n",
       "      <td>2.246866e+09</td>\n",
       "      <td>stormcrowalehouse</td>\n",
       "      <td>1.461802e+09</td>\n",
       "      <td>Just in case you thought you could get away wi...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3631.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.915</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.3818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1.644339e+09</td>\n",
       "      <td>4.325633e+09</td>\n",
       "      <td>laurencallenderz</td>\n",
       "      <td>1.570685e+09</td>\n",
       "      <td>üÖ•üÖêüÖüüÖî üÖòüÖ¢ üÖùüÖûüÖ£ üÖíüÖûüÖûüÖõ üÖ¢üÖû üÖìüÖûüÖùüÖ£ üÖôüÖ§üÖ§üÖõ I use to vape be...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>194.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.773</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.2960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1.644339e+09</td>\n",
       "      <td>9.235050e+09</td>\n",
       "      <td>sphsvapes</td>\n",
       "      <td>1.542394e+09</td>\n",
       "      <td>Asking the big questions #stopvaping</td>\n",
       "      <td>3.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8967</th>\n",
       "      <td>8967</td>\n",
       "      <td>8967</td>\n",
       "      <td>1.644339e+09</td>\n",
       "      <td>2.347172e+10</td>\n",
       "      <td>southyarracounselling</td>\n",
       "      <td>1.573712e+09</td>\n",
       "      <td>Now that vaping is being banned by so many, pe...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.742</td>\n",
       "      <td>0.127</td>\n",
       "      <td>-0.0258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8968</th>\n",
       "      <td>8968</td>\n",
       "      <td>8968</td>\n",
       "      <td>1.644339e+09</td>\n",
       "      <td>8.090645e+09</td>\n",
       "      <td>skylartatu</td>\n",
       "      <td>1.550698e+09</td>\n",
       "      <td>#WednesdayWisdom\\nStop trying to be perfect an...</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1089.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.819</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.9314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8969</th>\n",
       "      <td>8969</td>\n",
       "      <td>8969</td>\n",
       "      <td>1.644339e+09</td>\n",
       "      <td>2.205220e+09</td>\n",
       "      <td>allencarrseasyway</td>\n",
       "      <td>1.574772e+09</td>\n",
       "      <td>John Dicey, Global CEO of Allen Carr‚Äôs Easyway...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1433.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.821</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.3182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8970</th>\n",
       "      <td>8970</td>\n",
       "      <td>8970</td>\n",
       "      <td>1.644339e+09</td>\n",
       "      <td>4.779968e+08</td>\n",
       "      <td>doughboy0043</td>\n",
       "      <td>1.441838e+09</td>\n",
       "      <td>1st mile 9:35??? What??? That has gotta be a P...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.863</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.6958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8971</th>\n",
       "      <td>8971</td>\n",
       "      <td>8971</td>\n",
       "      <td>1.644339e+09</td>\n",
       "      <td>3.038993e+09</td>\n",
       "      <td>maxwell_di_paolo_photography</td>\n",
       "      <td>1.458941e+09</td>\n",
       "      <td>Candid of Steve and Sam #photo ‚ù§Ô∏è #friends #bw...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>310.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8939 rows √ó 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  Unnamed: 0.1  crawl_date_x       user_id  \\\n",
       "0              0             0  1.644339e+09  2.960596e+08   \n",
       "1              1             1  1.644339e+09  2.320914e+09   \n",
       "2              2             2  1.644339e+09  2.246866e+09   \n",
       "3              3             3  1.644339e+09  4.325633e+09   \n",
       "4              4             4  1.644339e+09  9.235050e+09   \n",
       "...          ...           ...           ...           ...   \n",
       "8967        8967          8967  1.644339e+09  2.347172e+10   \n",
       "8968        8968          8968  1.644339e+09  8.090645e+09   \n",
       "8969        8969          8969  1.644339e+09  2.205220e+09   \n",
       "8970        8970          8970  1.644339e+09  4.779968e+08   \n",
       "8971        8971          8971  1.644339e+09  3.038993e+09   \n",
       "\n",
       "                          username          date  \\\n",
       "0                 usernamesuckidge  1.563923e+09   \n",
       "1                   holisticatedme  1.465754e+09   \n",
       "2                stormcrowalehouse  1.461802e+09   \n",
       "3                 laurencallenderz  1.570685e+09   \n",
       "4                        sphsvapes  1.542394e+09   \n",
       "...                            ...           ...   \n",
       "8967         southyarracounselling  1.573712e+09   \n",
       "8968                    skylartatu  1.550698e+09   \n",
       "8969             allencarrseasyway  1.574772e+09   \n",
       "8970                  doughboy0043  1.441838e+09   \n",
       "8971  maxwell_di_paolo_photography  1.458941e+09   \n",
       "\n",
       "                                                caption  comment_count  \\\n",
       "0     #signofthetimes #welcometo2019 #nosmoking #nov...            0.0   \n",
       "1     Whenever I see someone smoking I say you know ...            3.0   \n",
       "2     Just in case you thought you could get away wi...            1.0   \n",
       "3     üÖ•üÖêüÖüüÖî üÖòüÖ¢ üÖùüÖûüÖ£ üÖíüÖûüÖûüÖõ üÖ¢üÖû üÖìüÖûüÖùüÖ£ üÖôüÖ§üÖ§üÖõ I use to vape be...            7.0   \n",
       "4                  Asking the big questions #stopvaping            3.0   \n",
       "...                                                 ...            ...   \n",
       "8967  Now that vaping is being banned by so many, pe...            1.0   \n",
       "8968  #WednesdayWisdom\\nStop trying to be perfect an...           16.0   \n",
       "8969  John Dicey, Global CEO of Allen Carr‚Äôs Easyway...            1.0   \n",
       "8970  1st mile 9:35??? What??? That has gotta be a P...            4.0   \n",
       "8971  Candid of Steve and Sam #photo ‚ù§Ô∏è #friends #bw...            1.0   \n",
       "\n",
       "      follower_count  like_count  ...  Old Product Sign  Text  Cartoon  \\\n",
       "0              280.0        16.0  ...    0       0    0     0        0   \n",
       "1              706.0        25.0  ...    0       0    0     1        0   \n",
       "2             3631.0        46.0  ...    0       0    0     1        0   \n",
       "3              194.0        23.0  ...    0       0    0     0        0   \n",
       "4              236.0        58.0  ...    0       0    0     0        0   \n",
       "...              ...         ...  ...  ...     ...  ...   ...      ...   \n",
       "8967            25.0         5.0  ...    0       0    0     0        0   \n",
       "8968          1089.0        47.0  ...    0       0    0     1        0   \n",
       "8969          1433.0        13.0  ...    0       0    0     1        0   \n",
       "8970           120.0        27.0  ...    0       0    0     1        0   \n",
       "8971           310.0        21.0  ...    0       1    0     0        0   \n",
       "\n",
       "     statistics  negativity  neutrality  positivity  compound  \n",
       "0             0       0.000       1.000       0.000    0.0000  \n",
       "1             0       0.072       0.829       0.099    0.3535  \n",
       "2             0       0.000       0.915       0.085    0.3818  \n",
       "3             0       0.112       0.773       0.115    0.2960  \n",
       "4             0       0.000       1.000       0.000    0.0000  \n",
       "...         ...         ...         ...         ...       ...  \n",
       "8967          0       0.131       0.742       0.127   -0.0258  \n",
       "8968          0       0.052       0.819       0.129    0.9314  \n",
       "8969          0       0.045       0.821       0.133    0.3182  \n",
       "8970          0       0.137       0.863       0.000   -0.6958  \n",
       "8971          0       0.000       1.000       0.000    0.0000  \n",
       "\n",
       "[8939 rows x 37 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8911f134",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df=reviews_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "cef7a8e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>crawl_date_x</th>\n",
       "      <th>user_id</th>\n",
       "      <th>username</th>\n",
       "      <th>date</th>\n",
       "      <th>caption</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>follower_count</th>\n",
       "      <th>like_count</th>\n",
       "      <th>...</th>\n",
       "      <th>Old</th>\n",
       "      <th>Product</th>\n",
       "      <th>Sign</th>\n",
       "      <th>Text</th>\n",
       "      <th>Cartoon</th>\n",
       "      <th>statistics</th>\n",
       "      <th>negativity</th>\n",
       "      <th>neutrality</th>\n",
       "      <th>positivity</th>\n",
       "      <th>compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.644339e+09</td>\n",
       "      <td>2.960596e+08</td>\n",
       "      <td>usernamesuckidge</td>\n",
       "      <td>1.563923e+09</td>\n",
       "      <td>#signofthetimes #welcometo2019 #nosmoking #nov...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.644339e+09</td>\n",
       "      <td>2.320914e+09</td>\n",
       "      <td>holisticatedme</td>\n",
       "      <td>1.465754e+09</td>\n",
       "      <td>Whenever I see someone smoking I say you know ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>706.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.829</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.3535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.644339e+09</td>\n",
       "      <td>2.246866e+09</td>\n",
       "      <td>stormcrowalehouse</td>\n",
       "      <td>1.461802e+09</td>\n",
       "      <td>Just in case you thought you could get away wi...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3631.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.915</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.3818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1.644339e+09</td>\n",
       "      <td>4.325633e+09</td>\n",
       "      <td>laurencallenderz</td>\n",
       "      <td>1.570685e+09</td>\n",
       "      <td>üÖ•üÖêüÖüüÖî üÖòüÖ¢ üÖùüÖûüÖ£ üÖíüÖûüÖûüÖõ üÖ¢üÖû üÖìüÖûüÖùüÖ£ üÖôüÖ§üÖ§üÖõ I use to vape be...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>194.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.773</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.2960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1.644339e+09</td>\n",
       "      <td>9.235050e+09</td>\n",
       "      <td>sphsvapes</td>\n",
       "      <td>1.542394e+09</td>\n",
       "      <td>Asking the big questions #stopvaping</td>\n",
       "      <td>3.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8934</th>\n",
       "      <td>8967</td>\n",
       "      <td>8967</td>\n",
       "      <td>1.644339e+09</td>\n",
       "      <td>2.347172e+10</td>\n",
       "      <td>southyarracounselling</td>\n",
       "      <td>1.573712e+09</td>\n",
       "      <td>Now that vaping is being banned by so many, pe...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.742</td>\n",
       "      <td>0.127</td>\n",
       "      <td>-0.0258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8935</th>\n",
       "      <td>8968</td>\n",
       "      <td>8968</td>\n",
       "      <td>1.644339e+09</td>\n",
       "      <td>8.090645e+09</td>\n",
       "      <td>skylartatu</td>\n",
       "      <td>1.550698e+09</td>\n",
       "      <td>#WednesdayWisdom\\nStop trying to be perfect an...</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1089.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.819</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.9314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8936</th>\n",
       "      <td>8969</td>\n",
       "      <td>8969</td>\n",
       "      <td>1.644339e+09</td>\n",
       "      <td>2.205220e+09</td>\n",
       "      <td>allencarrseasyway</td>\n",
       "      <td>1.574772e+09</td>\n",
       "      <td>John Dicey, Global CEO of Allen Carr‚Äôs Easyway...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1433.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.821</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.3182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8937</th>\n",
       "      <td>8970</td>\n",
       "      <td>8970</td>\n",
       "      <td>1.644339e+09</td>\n",
       "      <td>4.779968e+08</td>\n",
       "      <td>doughboy0043</td>\n",
       "      <td>1.441838e+09</td>\n",
       "      <td>1st mile 9:35??? What??? That has gotta be a P...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.863</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.6958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8938</th>\n",
       "      <td>8971</td>\n",
       "      <td>8971</td>\n",
       "      <td>1.644339e+09</td>\n",
       "      <td>3.038993e+09</td>\n",
       "      <td>maxwell_di_paolo_photography</td>\n",
       "      <td>1.458941e+09</td>\n",
       "      <td>Candid of Steve and Sam #photo ‚ù§Ô∏è #friends #bw...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>310.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8939 rows √ó 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  Unnamed: 0.1  crawl_date_x       user_id  \\\n",
       "0              0             0  1.644339e+09  2.960596e+08   \n",
       "1              1             1  1.644339e+09  2.320914e+09   \n",
       "2              2             2  1.644339e+09  2.246866e+09   \n",
       "3              3             3  1.644339e+09  4.325633e+09   \n",
       "4              4             4  1.644339e+09  9.235050e+09   \n",
       "...          ...           ...           ...           ...   \n",
       "8934        8967          8967  1.644339e+09  2.347172e+10   \n",
       "8935        8968          8968  1.644339e+09  8.090645e+09   \n",
       "8936        8969          8969  1.644339e+09  2.205220e+09   \n",
       "8937        8970          8970  1.644339e+09  4.779968e+08   \n",
       "8938        8971          8971  1.644339e+09  3.038993e+09   \n",
       "\n",
       "                          username          date  \\\n",
       "0                 usernamesuckidge  1.563923e+09   \n",
       "1                   holisticatedme  1.465754e+09   \n",
       "2                stormcrowalehouse  1.461802e+09   \n",
       "3                 laurencallenderz  1.570685e+09   \n",
       "4                        sphsvapes  1.542394e+09   \n",
       "...                            ...           ...   \n",
       "8934         southyarracounselling  1.573712e+09   \n",
       "8935                    skylartatu  1.550698e+09   \n",
       "8936             allencarrseasyway  1.574772e+09   \n",
       "8937                  doughboy0043  1.441838e+09   \n",
       "8938  maxwell_di_paolo_photography  1.458941e+09   \n",
       "\n",
       "                                                caption  comment_count  \\\n",
       "0     #signofthetimes #welcometo2019 #nosmoking #nov...            0.0   \n",
       "1     Whenever I see someone smoking I say you know ...            3.0   \n",
       "2     Just in case you thought you could get away wi...            1.0   \n",
       "3     üÖ•üÖêüÖüüÖî üÖòüÖ¢ üÖùüÖûüÖ£ üÖíüÖûüÖûüÖõ üÖ¢üÖû üÖìüÖûüÖùüÖ£ üÖôüÖ§üÖ§üÖõ I use to vape be...            7.0   \n",
       "4                  Asking the big questions #stopvaping            3.0   \n",
       "...                                                 ...            ...   \n",
       "8934  Now that vaping is being banned by so many, pe...            1.0   \n",
       "8935  #WednesdayWisdom\\nStop trying to be perfect an...           16.0   \n",
       "8936  John Dicey, Global CEO of Allen Carr‚Äôs Easyway...            1.0   \n",
       "8937  1st mile 9:35??? What??? That has gotta be a P...            4.0   \n",
       "8938  Candid of Steve and Sam #photo ‚ù§Ô∏è #friends #bw...            1.0   \n",
       "\n",
       "      follower_count  like_count  ...  Old Product Sign  Text  Cartoon  \\\n",
       "0              280.0        16.0  ...    0       0    0     0        0   \n",
       "1              706.0        25.0  ...    0       0    0     1        0   \n",
       "2             3631.0        46.0  ...    0       0    0     1        0   \n",
       "3              194.0        23.0  ...    0       0    0     0        0   \n",
       "4              236.0        58.0  ...    0       0    0     0        0   \n",
       "...              ...         ...  ...  ...     ...  ...   ...      ...   \n",
       "8934            25.0         5.0  ...    0       0    0     0        0   \n",
       "8935          1089.0        47.0  ...    0       0    0     1        0   \n",
       "8936          1433.0        13.0  ...    0       0    0     1        0   \n",
       "8937           120.0        27.0  ...    0       0    0     1        0   \n",
       "8938           310.0        21.0  ...    0       1    0     0        0   \n",
       "\n",
       "     statistics  negativity  neutrality  positivity  compound  \n",
       "0             0       0.000       1.000       0.000    0.0000  \n",
       "1             0       0.072       0.829       0.099    0.3535  \n",
       "2             0       0.000       0.915       0.085    0.3818  \n",
       "3             0       0.112       0.773       0.115    0.2960  \n",
       "4             0       0.000       1.000       0.000    0.0000  \n",
       "...         ...         ...         ...         ...       ...  \n",
       "8934          0       0.131       0.742       0.127   -0.0258  \n",
       "8935          0       0.052       0.819       0.129    0.9314  \n",
       "8936          0       0.045       0.821       0.133    0.3182  \n",
       "8937          0       0.137       0.863       0.000   -0.6958  \n",
       "8938          0       0.000       1.000       0.000    0.0000  \n",
       "\n",
       "[8939 rows x 37 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d72c050e",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df.to_csv('8939_result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1955c751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A tool for creating instance lists of feature vectors from comma-separated-values\r\n",
      "--help TRUE|FALSE\r\n",
      "  Print this command line option usage information.  Give argument of TRUE for longer documentation\r\n",
      "  Default is false\r\n",
      "--prefix-code 'JAVA CODE'\r\n",
      "  Java code you want run before any other interpreted code.  Note that the text is interpreted without modification, so unlike some other Java code options, you need to include any necessary 'new's when creating objects.\r\n",
      "  Default is null\r\n",
      "--config FILE\r\n",
      "  Read command option values from a file\r\n",
      "  Default is null\r\n",
      "--input FILE\r\n",
      "  The file containing data to be classified, one instance per line\r\n",
      "  Default is null\r\n",
      "--output FILE\r\n",
      "  Write the instance list to this file; Using - indicates stdout.\r\n",
      "  Default is text.vectors\r\n",
      "--line-regex REGEX\r\n",
      "  Regular expression containing regex-groups for label, name and data.\r\n",
      "  Default is ^(\\S*)[\\s,]*(\\S*)[\\s,]*(.*)$\r\n",
      "--label INTEGER\r\n",
      "  The index of the group containing the label string.\r\n",
      "   Use 0 to indicate that the label field is not used.\r\n",
      "  Default is 2\r\n",
      "--name INTEGER\r\n",
      "  The index of the group containing the instance name.\r\n",
      "   Use 0 to indicate that the name field is not used.\r\n",
      "  Default is 1\r\n",
      "--data INTEGER\r\n",
      "  The index of the group containing the data.\r\n",
      "  Default is 3\r\n",
      "--use-pipe-from FILE\r\n",
      "  Use the pipe and alphabets from a previously created vectors file.\r\n",
      "   Allows the creation, for example, of a test set of vectors that are\r\n",
      "   compatible with a previously created set of training vectors\r\n",
      "  Default is text.vectors\r\n",
      "--keep-sequence [TRUE|FALSE]\r\n",
      "  If true, final data will be a FeatureSequence rather than a FeatureVector.\r\n",
      "  Default is false\r\n",
      "--keep-sequence-bigrams [TRUE|FALSE]\r\n",
      "  If true, final data will be a FeatureSequenceWithBigrams rather than a FeatureVector.\r\n",
      "  Default is false\r\n",
      "--label-as-features [TRUE|FALSE]\r\n",
      "  If true, parse the 'label' field as space-delimited features.\r\n",
      "     Use feature=[number] to specify values for non-binary features.\r\n",
      "  Default is false\r\n",
      "--remove-stopwords [TRUE|FALSE]\r\n",
      "  If true, remove a default list of common English \"stop words\" from the text.\r\n",
      "  Default is false\r\n",
      "--replacement-files FILE [FILE ...]\r\n",
      "  files containing string replacements, one per line:\r\n",
      "    'A B [tab] C' replaces A B with C,\r\n",
      "    'A B' replaces A B with A_B\r\n",
      "  Default is (null)\r\n",
      "--deletion-files FILE [FILE ...]\r\n",
      "  files containing strings to delete after replacements but before tokenization (ie multiword stop terms)\r\n",
      "  Default is (null)\r\n",
      "--stoplist-file FILE\r\n",
      "  Instead of the default list, read stop words from a file, one per line. Implies --remove-stopwords\r\n",
      "  Default is null\r\n",
      "--extra-stopwords FILE\r\n",
      "  Read whitespace-separated words from this file, and add them to either \r\n",
      "   the default English stoplist or the list specified by --stoplist-file.\r\n",
      "  Default is null\r\n",
      "--stop-pattern-file FILE\r\n",
      "  Read regular expressions from a file, one per line. Tokens matching these regexps will be removed.\r\n",
      "  Default is null\r\n",
      "--preserve-case [TRUE|FALSE]\r\n",
      "  If true, do not force all strings to lowercase.\r\n",
      "  Default is false\r\n",
      "--encoding STRING\r\n",
      "  Character encoding for input file\r\n",
      "  Default is UTF-8\r\n",
      "--token-regex REGEX\r\n",
      "  Regular expression used for tokenization.\r\n",
      "   Example: \"[\\p{L}\\p{N}_]+|[\\p{P}]+\" (unicode letters, numbers and underscore OR all punctuation) \r\n",
      "  Default is \\p{L}[\\p{L}\\p{P}]+\\p{L}\r\n",
      "--print-output [TRUE|FALSE]\r\n",
      "  If true, print a representation of the processed data\r\n",
      "   to standard output. This option is intended for debugging.\r\n",
      "  Default is false\r\n"
     ]
    }
   ],
   "source": [
    "!~/mallet-2.0.8/bin/mallet import-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8a9d1cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: little_mallet_wrapper in /Users/carol/opt/anaconda3/lib/python3.8/site-packages (0.5.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install little_mallet_wrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3063c987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in /Users/carol/opt/anaconda3/lib/python3.8/site-packages (0.11.2)\n",
      "Requirement already satisfied: matplotlib>=2.2 in /Users/carol/opt/anaconda3/lib/python3.8/site-packages (from seaborn) (3.5.0)\n",
      "Requirement already satisfied: scipy>=1.0 in /Users/carol/opt/anaconda3/lib/python3.8/site-packages (from seaborn) (1.7.1)\n",
      "Requirement already satisfied: pandas>=0.23 in /Users/carol/opt/anaconda3/lib/python3.8/site-packages (from seaborn) (1.3.4)\n",
      "Requirement already satisfied: numpy>=1.15 in /Users/carol/opt/anaconda3/lib/python3.8/site-packages (from seaborn) (1.20.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/carol/opt/anaconda3/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/carol/opt/anaconda3/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (1.3.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/carol/opt/anaconda3/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (8.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/carol/opt/anaconda3/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/carol/opt/anaconda3/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (3.0.4)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/carol/opt/anaconda3/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/carol/opt/anaconda3/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (21.3)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/carol/opt/anaconda3/lib/python3.8/site-packages (from pandas>=0.23->seaborn) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/carol/opt/anaconda3/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib>=2.2->seaborn) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e728977a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/carol/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m************LOADING DATA************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-20 13:33:34,561 : INFO : collecting all words and their counts\n",
      "2022-02-20 13:33:34,564 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\n",
      "********LOADING DATA COMPLETE*******\n",
      "\u001b[34m\n",
      "******Creating bigram and trigram******\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-20 13:33:35,054 : INFO : collected 185855 token types (unigram + bigrams) from a corpus of 321520 words and 8887 sentences\n",
      "2022-02-20 13:33:35,054 : INFO : merged Phrases<185855 vocab, min_count=5, threshold=100, max_vocab_size=40000000>\n",
      "2022-02-20 13:33:35,055 : INFO : Phrases lifecycle event {'msg': 'built Phrases<185855 vocab, min_count=5, threshold=100, max_vocab_size=40000000> in 0.49s', 'datetime': '2022-02-20T13:33:35.055296', 'gensim': '4.1.2', 'python': '3.8.11 (default, Aug  6 2021, 08:56:27) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.14.6-x86_64-i386-64bit', 'event': 'created'}\n",
      "2022-02-20 13:33:35,056 : INFO : collecting all words and their counts\n",
      "2022-02-20 13:33:35,057 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2022-02-20 13:33:36,182 : INFO : collected 187949 token types (unigram + bigrams) from a corpus of 293609 words and 8887 sentences\n",
      "2022-02-20 13:33:36,183 : INFO : merged Phrases<187949 vocab, min_count=5, threshold=100, max_vocab_size=40000000>\n",
      "2022-02-20 13:33:36,183 : INFO : Phrases lifecycle event {'msg': 'built Phrases<187949 vocab, min_count=5, threshold=100, max_vocab_size=40000000> in 1.13s', 'datetime': '2022-02-20T13:33:36.183715', 'gensim': '4.1.2', 'python': '3.8.11 (default, Aug  6 2021, 08:56:27) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.14.6-x86_64-i386-64bit', 'event': 'created'}\n",
      "2022-02-20 13:33:36,184 : INFO : exporting phrases from Phrases<185855 vocab, min_count=5, threshold=100, max_vocab_size=40000000>\n",
      "2022-02-20 13:33:36,535 : INFO : FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<1677 phrases, min_count=5, threshold=100> from Phrases<185855 vocab, min_count=5, threshold=100, max_vocab_size=40000000> in 0.35s', 'datetime': '2022-02-20T13:33:36.535451', 'gensim': '4.1.2', 'python': '3.8.11 (default, Aug  6 2021, 08:56:27) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.14.6-x86_64-i386-64bit', 'event': 'created'}\n",
      "2022-02-20 13:33:36,536 : INFO : exporting phrases from Phrases<187949 vocab, min_count=5, threshold=100, max_vocab_size=40000000>\n",
      "2022-02-20 13:33:36,889 : INFO : FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<1468 phrases, min_count=5, threshold=100> from Phrases<187949 vocab, min_count=5, threshold=100, max_vocab_size=40000000> in 0.35s', 'datetime': '2022-02-20T13:33:36.889951', 'gensim': '4.1.2', 'python': '3.8.11 (default, Aug  6 2021, 08:56:27) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.14.6-x86_64-i386-64bit', 'event': 'created'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\n",
      "******Finished creating bigram and trigram******\n",
      "\n",
      "\u001b[34m******Start fine preprocess******\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-20 13:34:10,439 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2022-02-20 13:34:10,626 : INFO : built Dictionary(21122 unique tokens: ['bcpark', 'nosmoke', 'novape', 'signofthetime', 'welcometo']...) from 8887 documents (total 144357 corpus positions)\n",
      "2022-02-20 13:34:10,627 : INFO : Dictionary lifecycle event {'msg': \"built Dictionary(21122 unique tokens: ['bcpark', 'nosmoke', 'novape', 'signofthetime', 'welcometo']...) from 8887 documents (total 144357 corpus positions)\", 'datetime': '2022-02-20T13:34:10.627607', 'gensim': '4.1.2', 'python': '3.8.11 (default, Aug  6 2021, 08:56:27) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.14.6-x86_64-i386-64bit', 'event': 'created'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m******Finished fine preprocess******\n",
      "\n",
      "\u001b[34m******Start creating dictionary and corpus for LDA******\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-20 13:34:12,107 : INFO : using symmetric alpha at 0.125\n",
      "2022-02-20 13:34:12,107 : INFO : using symmetric eta at 0.125\n",
      "2022-02-20 13:34:12,111 : INFO : using serial LDA version on this node\n",
      "2022-02-20 13:34:12,127 : INFO : running online (single-pass) LDA training, 8 topics, 1 passes over the supplied corpus of 8887 documents, updating model once every 2000 documents, evaluating perplexity every 8887 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2022-02-20 13:34:12,127 : WARNING : too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      "2022-02-20 13:34:12,128 : INFO : PROGRESS: pass 0, at document #2000/8887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m******Finished creating dictionary and corpus for LDA******\n",
      "\n",
      "\u001b[34m******Start building LDA model******\n",
      "\n",
      "Number of topic is:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-20 13:34:12,858 : INFO : merging changes from 2000 documents into a model of 8887 documents\n",
      "2022-02-20 13:34:12,873 : INFO : topic #4 (0.125): 0.018*\"novape\" + 0.015*\"stopvape\" + 0.012*\"vape\" + 0.008*\"stopvaping\" + 0.008*\"nosmoke\" + 0.007*\"stopsmoke\" + 0.006*\"get\" + 0.006*\"stop\" + 0.006*\"cigarette\" + 0.005*\"go\"\n",
      "2022-02-20 13:34:12,874 : INFO : topic #7 (0.125): 0.020*\"vape\" + 0.020*\"smoke\" + 0.016*\"stopvape\" + 0.014*\"stopvaping\" + 0.011*\"stopsmoke\" + 0.010*\"novape\" + 0.007*\"tobacco\" + 0.007*\"smoking\" + 0.007*\"people\" + 0.006*\"stop\"\n",
      "2022-02-20 13:34:12,876 : INFO : topic #6 (0.125): 0.042*\"vape\" + 0.021*\"novape\" + 0.018*\"stopvape\" + 0.016*\"cigarette\" + 0.015*\"stopvaping\" + 0.011*\"smoke\" + 0.010*\"nosmoke\" + 0.009*\"quit\" + 0.008*\"stop\" + 0.008*\"nicotine\"\n",
      "2022-02-20 13:34:12,877 : INFO : topic #2 (0.125): 0.036*\"vape\" + 0.017*\"novape\" + 0.012*\"smoke\" + 0.010*\"nosmoke\" + 0.009*\"stopvape\" + 0.008*\"stopvaping\" + 0.007*\"get\" + 0.007*\"smoking\" + 0.006*\"nicotine\" + 0.006*\"stop\"\n",
      "2022-02-20 13:34:12,878 : INFO : topic #1 (0.125): 0.030*\"vape\" + 0.016*\"novape\" + 0.014*\"stopvaping\" + 0.010*\"stopvape\" + 0.008*\"nicotine\" + 0.007*\"know\" + 0.007*\"smoke\" + 0.006*\"make\" + 0.006*\"day\" + 0.005*\"novaping\"\n",
      "2022-02-20 13:34:12,878 : INFO : topic diff=6.958020, rho=1.000000\n",
      "2022-02-20 13:34:12,880 : INFO : PROGRESS: pass 0, at document #4000/8887\n",
      "2022-02-20 13:34:13,387 : INFO : merging changes from 2000 documents into a model of 8887 documents\n",
      "2022-02-20 13:34:13,394 : INFO : topic #5 (0.125): 0.028*\"√∞¬æ√∞\" + 0.018*\"novape\" + 0.018*\"√∞¬µ√∞\" + 0.018*\"√∞¬æ\" + 0.018*\"√∞¬µn\" + 0.018*\"√∞¬º√∞\" + 0.016*\"√∞¬µ\" + 0.014*\"√∞¬∫√∞\" + 0.011*\"√∞¬∫n\" + 0.011*\"nosmoke\"\n",
      "2022-02-20 13:34:13,395 : INFO : topic #3 (0.125): 0.037*\"novape\" + 0.016*\"vape\" + 0.013*\"stopvaping\" + 0.013*\"nosmoke\" + 0.012*\"smoke\" + 0.007*\"stopvape\" + 0.006*\"tobacco\" + 0.006*\"cloud\" + 0.005*\"know\" + 0.005*\"look\"\n",
      "2022-02-20 13:34:13,396 : INFO : topic #1 (0.125): 0.028*\"vape\" + 0.021*\"stopvaping\" + 0.015*\"novape\" + 0.012*\"oml\" + 0.011*\"dm_lingle\" + 0.010*\"soon_toosoon\" + 0.008*\"day\" + 0.008*\"stopvape\" + 0.007*\"nicotine\" + 0.006*\"get\"\n",
      "2022-02-20 13:34:13,397 : INFO : topic #4 (0.125): 0.016*\"novape\" + 0.014*\"stopvape\" + 0.010*\"vape\" + 0.007*\"get\" + 0.007*\"nosmoke\" + 0.007*\"go\" + 0.006*\"stopvaping\" + 0.005*\"stopsmoke\" + 0.005*\"time\" + 0.005*\"stop\"\n",
      "2022-02-20 13:34:13,398 : INFO : topic #7 (0.125): 0.020*\"smoke\" + 0.018*\"vape\" + 0.015*\"stopvaping\" + 0.013*\"stopvape\" + 0.011*\"stopsmoke\" + 0.009*\"novape\" + 0.008*\"smoking\" + 0.007*\"people\" + 0.007*\"stop\" + 0.006*\"yang_kecil\"\n",
      "2022-02-20 13:34:13,399 : INFO : topic diff=0.599411, rho=0.707107\n",
      "2022-02-20 13:34:13,400 : INFO : PROGRESS: pass 0, at document #6000/8887\n",
      "2022-02-20 13:34:13,826 : INFO : merging changes from 2000 documents into a model of 8887 documents\n",
      "2022-02-20 13:34:13,834 : INFO : topic #6 (0.125): 0.044*\"vape\" + 0.021*\"quit\" + 0.016*\"cigarette\" + 0.014*\"novape\" + 0.012*\"nicotine\" + 0.012*\"smoke\" + 0.011*\"health\" + 0.010*\"stopvape\" + 0.008*\"antivape\" + 0.008*\"stop\"\n",
      "2022-02-20 13:34:13,835 : INFO : topic #2 (0.125): 0.034*\"vape\" + 0.025*\"antivape\" + 0.015*\"novape\" + 0.012*\"smoke\" + 0.009*\"get\" + 0.006*\"nosmoke\" + 0.006*\"go\" + 0.006*\"friend\" + 0.005*\"smoking\" + 0.005*\"scary_antivape\"\n",
      "2022-02-20 13:34:13,836 : INFO : topic #7 (0.125): 0.017*\"smoke\" + 0.012*\"vape\" + 0.009*\"stopvaping\" + 0.008*\"stopvape\" + 0.007*\"stopsmoke\" + 0.007*\"novape\" + 0.006*\"smoking\" + 0.006*\"people\" + 0.005*\"day\" + 0.005*\"stop\"\n",
      "2022-02-20 13:34:13,836 : INFO : topic #1 (0.125): 0.025*\"vape\" + 0.012*\"scary_antivape\" + 0.012*\"nicotine\" + 0.011*\"day\" + 0.010*\"stopvaping\" + 0.009*\"novape\" + 0.007*\"antismoking_antivape\" + 0.007*\"get\" + 0.007*\"memes_tidepod\" + 0.006*\"go\"\n",
      "2022-02-20 13:34:13,837 : INFO : topic #5 (0.125): 0.017*\"√∞¬æ√∞\" + 0.015*\"novape\" + 0.013*\"app_cegahnarkoba\" + 0.013*\"sekarang\" + 0.012*\"√∞¬µ√∞\" + 0.012*\"nosmoke\" + 0.011*\"√∞¬æ\" + 0.011*\"√∞¬µn\" + 0.010*\"√∞¬º√∞\" + 0.010*\"√∞¬∫√∞\"\n",
      "2022-02-20 13:34:13,838 : INFO : topic diff=0.580891, rho=0.577350\n",
      "2022-02-20 13:34:13,839 : INFO : PROGRESS: pass 0, at document #8000/8887\n",
      "2022-02-20 13:34:14,207 : INFO : merging changes from 2000 documents into a model of 8887 documents\n",
      "2022-02-20 13:34:14,215 : INFO : topic #7 (0.125): 0.014*\"smoke\" + 0.009*\"vape\" + 0.006*\"stopvape\" + 0.006*\"stopsmoke\" + 0.005*\"stopvaping\" + 0.005*\"novape\" + 0.005*\"smoking\" + 0.004*\"day\" + 0.004*\"water\" + 0.004*\"quitsmoking\"\n",
      "2022-02-20 13:34:14,216 : INFO : topic #6 (0.125): 0.046*\"vape\" + 0.020*\"quit\" + 0.017*\"cigarette\" + 0.017*\"dontvape\" + 0.013*\"nicotine\" + 0.012*\"smoke\" + 0.011*\"health\" + 0.009*\"smoking\" + 0.009*\"tobacco\" + 0.008*\"quitvape\"\n",
      "2022-02-20 13:34:14,217 : INFO : topic #2 (0.125): 0.031*\"vape\" + 0.018*\"antivape\" + 0.015*\"meme\" + 0.014*\"dontvape\" + 0.010*\"smoke\" + 0.009*\"follow\" + 0.009*\"novape\" + 0.009*\"get\" + 0.007*\"funny\" + 0.006*\"go\"\n",
      "2022-02-20 13:34:14,219 : INFO : topic #0 (0.125): 0.034*\"novape\" + 0.032*\"scary_antivape\" + 0.031*\"vape\" + 0.030*\"memes_tidepod\" + 0.025*\"driving_spooky\" + 0.018*\"nosmoke\" + 0.014*\"jimmyneutron_nfl\" + 0.014*\"ricardo_meandtheboy\" + 0.014*\"world_carlwheezer\" + 0.014*\"memehouse\"\n",
      "2022-02-20 13:34:14,220 : INFO : topic #5 (0.125): 0.013*\"novape\" + 0.012*\"app_cegahnarkoba\" + 0.011*\"sekarang\" + 0.011*\"nosmoke\" + 0.011*\"√∞¬æ√∞\" + 0.008*\"√∞¬µ√∞\" + 0.007*\"√∞¬æ\" + 0.007*\"antivape_rokokelektrik\" + 0.007*\"√∞¬µn\" + 0.007*\"√∞¬º√∞\"\n",
      "2022-02-20 13:34:14,220 : INFO : topic diff=0.556495, rho=0.500000\n",
      "2022-02-20 13:34:14,507 : INFO : -9.056 per-word bound, 532.4 perplexity estimate based on a held-out corpus of 887 documents with 23796 words\n",
      "2022-02-20 13:34:14,508 : INFO : PROGRESS: pass 0, at document #8887/8887\n",
      "2022-02-20 13:34:14,700 : INFO : merging changes from 887 documents into a model of 8887 documents\n",
      "2022-02-20 13:34:14,708 : INFO : topic #1 (0.125): 0.021*\"vape\" + 0.014*\"day\" + 0.009*\"nicotine\" + 0.008*\"quitnicotine\" + 0.008*\"get\" + 0.007*\"go\" + 0.007*\"good\" + 0.007*\"stopvaping\" + 0.006*\"run\" + 0.006*\"_\"\n",
      "2022-02-20 13:34:14,709 : INFO : topic #7 (0.125): 0.013*\"smoke\" + 0.012*\"quitsmoking\" + 0.009*\"stopsmoke\" + 0.008*\"quitvaping\" + 0.007*\"skincare\" + 0.007*\"vape\" + 0.007*\"hypnosis\" + 0.006*\"stopvape\" + 0.006*\"definitely\" + 0.006*\"beauty\"\n",
      "2022-02-20 13:34:14,710 : INFO : topic #6 (0.125): 0.045*\"vape\" + 0.024*\"quit\" + 0.023*\"quitvape\" + 0.017*\"nicotine\" + 0.016*\"cigarette\" + 0.013*\"smoke\" + 0.012*\"health\" + 0.011*\"quitsmoke\" + 0.011*\"smoking\" + 0.010*\"help\"\n",
      "2022-02-20 13:34:14,711 : INFO : topic #2 (0.125): 0.026*\"vape\" + 0.012*\"antivape\" + 0.011*\"meme\" + 0.010*\"get\" + 0.008*\"smoke\" + 0.008*\"go\" + 0.008*\"dontvape\" + 0.007*\"want\" + 0.006*\"funny\" + 0.006*\"follow\"\n",
      "2022-02-20 13:34:14,713 : INFO : topic #4 (0.125): 0.015*\"quitvaping\" + 0.013*\"quitsmoke\" + 0.011*\"hypnosis_hypnotherapy\" + 0.007*\"journey\" + 0.006*\"way\" + 0.006*\"get\" + 0.006*\"go\" + 0.005*\"time\" + 0.005*\"natural\" + 0.005*\"potential\"\n",
      "2022-02-20 13:34:14,714 : INFO : topic diff=0.566091, rho=0.447214\n",
      "2022-02-20 13:34:14,715 : INFO : LdaModel lifecycle event {'msg': 'trained LdaModel(num_terms=21122, num_topics=8, decay=0.5, chunksize=2000) in 2.59s', 'datetime': '2022-02-20T13:34:14.715416', 'gensim': '4.1.2', 'python': '3.8.11 (default, Aug  6 2021, 08:56:27) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.14.6-x86_64-i386-64bit', 'event': 'created'}\n",
      "2022-02-20 13:34:14,721 : INFO : LdaState lifecycle event {'fname_or_handle': '/Users/carol/opt/anaconda3/lib/python3.8/site-packages/gensim/test/test_data/model_8.state', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2022-02-20T13:34:14.721620', 'gensim': '4.1.2', 'python': '3.8.11 (default, Aug  6 2021, 08:56:27) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.14.6-x86_64-i386-64bit', 'event': 'saving'}\n",
      "2022-02-20 13:34:14,724 : INFO : saved /Users/carol/opt/anaconda3/lib/python3.8/site-packages/gensim/test/test_data/model_8.state\n",
      "2022-02-20 13:34:14,736 : INFO : LdaModel lifecycle event {'fname_or_handle': '/Users/carol/opt/anaconda3/lib/python3.8/site-packages/gensim/test/test_data/model_8', 'separately': \"['expElogbeta', 'sstats']\", 'sep_limit': 10485760, 'ignore': ['state', 'dispatcher', 'id2word'], 'datetime': '2022-02-20T13:34:14.735991', 'gensim': '4.1.2', 'python': '3.8.11 (default, Aug  6 2021, 08:56:27) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.14.6-x86_64-i386-64bit', 'event': 'saving'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-20 13:34:14,737 : INFO : storing np array 'expElogbeta' to /Users/carol/opt/anaconda3/lib/python3.8/site-packages/gensim/test/test_data/model_8.expElogbeta.npy\n",
      "2022-02-20 13:34:14,742 : INFO : not storing attribute state\n",
      "2022-02-20 13:34:14,743 : INFO : not storing attribute dispatcher\n",
      "2022-02-20 13:34:14,744 : INFO : not storing attribute id2word\n",
      "2022-02-20 13:34:14,745 : INFO : saved /Users/carol/opt/anaconda3/lib/python3.8/site-packages/gensim/test/test_data/model_8\n",
      "2022-02-20 13:34:14,749 : INFO : using ParallelWordOccurrenceAccumulator(processes=7, batch_size=64) to estimate probabilities from sliding windows\n",
      "2022-02-20 13:34:20,611 : INFO : 7 accumulators retrieved from output queue\n",
      "2022-02-20 13:34:20,633 : INFO : accumulated word occurrence stats for 9618 virtual documents\n",
      "2022-02-20 13:34:20,922 : INFO : using symmetric alpha at 0.1111111111111111\n",
      "2022-02-20 13:34:20,922 : INFO : using symmetric eta at 0.1111111111111111\n",
      "2022-02-20 13:34:20,926 : INFO : using serial LDA version on this node\n",
      "2022-02-20 13:34:20,943 : INFO : running online (single-pass) LDA training, 9 topics, 1 passes over the supplied corpus of 8887 documents, updating model once every 2000 documents, evaluating perplexity every 8887 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2022-02-20 13:34:20,944 : WARNING : too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      "2022-02-20 13:34:20,946 : INFO : PROGRESS: pass 0, at document #2000/8887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of topic is:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-20 13:34:21,628 : INFO : merging changes from 2000 documents into a model of 8887 documents\n",
      "2022-02-20 13:34:21,637 : INFO : topic #0 (0.111): 0.042*\"vape\" + 0.019*\"novape\" + 0.010*\"stopvape\" + 0.009*\"stopvaping\" + 0.008*\"smoke\" + 0.008*\"cigarette\" + 0.007*\"nosmoke\" + 0.007*\"stop\" + 0.006*\"day\" + 0.006*\"stopsmoke\"\n",
      "2022-02-20 13:34:21,639 : INFO : topic #1 (0.111): 0.033*\"vape\" + 0.015*\"stopvape\" + 0.012*\"smoke\" + 0.011*\"know\" + 0.011*\"novape\" + 0.008*\"cigarette\" + 0.008*\"life\" + 0.008*\"stopvaping\" + 0.007*\"take\" + 0.007*\"nosmoke\"\n",
      "2022-02-20 13:34:21,640 : INFO : topic #2 (0.111): 0.044*\"vape\" + 0.018*\"novape\" + 0.011*\"stopvape\" + 0.011*\"cigarette\" + 0.010*\"nosmoke\" + 0.009*\"stopvaping\" + 0.009*\"smoke\" + 0.009*\"quit\" + 0.009*\"health\" + 0.008*\"stop\"\n",
      "2022-02-20 13:34:21,641 : INFO : topic #3 (0.111): 0.042*\"novape\" + 0.014*\"vape\" + 0.013*\"nosmoke\" + 0.005*\"stopvape\" + 0.005*\"novaping\" + 0.005*\"say\" + 0.005*\"know\" + 0.004*\"today\" + 0.004*\"time\" + 0.004*\"get\"\n",
      "2022-02-20 13:34:21,643 : INFO : topic #8 (0.111): 0.035*\"vape\" + 0.030*\"novape\" + 0.015*\"smoke\" + 0.013*\"stopvaping\" + 0.013*\"stopvape\" + 0.012*\"cigarette\" + 0.008*\"nosmoke\" + 0.007*\"teen\" + 0.006*\"life\" + 0.006*\"health\"\n",
      "2022-02-20 13:34:21,644 : INFO : topic diff=7.840984, rho=1.000000\n",
      "2022-02-20 13:34:21,646 : INFO : PROGRESS: pass 0, at document #4000/8887\n",
      "2022-02-20 13:34:22,097 : INFO : merging changes from 2000 documents into a model of 8887 documents\n",
      "2022-02-20 13:34:22,106 : INFO : topic #4 (0.111): 0.019*\"vape\" + 0.017*\"stopvaping\" + 0.011*\"novape\" + 0.009*\"stopvape\" + 0.008*\"get\" + 0.007*\"cigarette\" + 0.007*\"friend\" + 0.006*\"go\" + 0.006*\"antivape\" + 0.006*\"year\"\n",
      "2022-02-20 13:34:22,107 : INFO : topic #2 (0.111): 0.048*\"vape\" + 0.017*\"quit\" + 0.017*\"novape\" + 0.013*\"cigarette\" + 0.012*\"stopvape\" + 0.012*\"health\" + 0.011*\"smoke\" + 0.010*\"stop\" + 0.010*\"smoking\" + 0.010*\"tobacco\"\n",
      "2022-02-20 13:34:22,108 : INFO : topic #0 (0.111): 0.049*\"vape\" + 0.020*\"novape\" + 0.010*\"stopvape\" + 0.009*\"cigarette\" + 0.008*\"stop\" + 0.007*\"stopvaping\" + 0.007*\"people\" + 0.007*\"smoke\" + 0.007*\"day\" + 0.007*\"nosmoke\"\n",
      "2022-02-20 13:34:22,109 : INFO : topic #6 (0.111): 0.026*\"novape\" + 0.022*\"vape\" + 0.015*\"nosmoke\" + 0.014*\"smoke\" + 0.011*\"get\" + 0.011*\"stop\" + 0.011*\"stopvape\" + 0.008*\"sign\" + 0.008*\"smoking\" + 0.007*\"time\"\n",
      "2022-02-20 13:34:22,110 : INFO : topic #1 (0.111): 0.039*\"vape\" + 0.017*\"stopvape\" + 0.011*\"know\" + 0.010*\"nicotine\" + 0.010*\"novape\" + 0.009*\"smoke\" + 0.009*\"cigarette\" + 0.007*\"life\" + 0.007*\"stopvaping\" + 0.007*\"tobacco\"\n",
      "2022-02-20 13:34:22,111 : INFO : topic diff=0.466981, rho=0.707107\n",
      "2022-02-20 13:34:22,112 : INFO : PROGRESS: pass 0, at document #6000/8887\n",
      "2022-02-20 13:34:22,498 : INFO : merging changes from 2000 documents into a model of 8887 documents\n",
      "2022-02-20 13:34:22,506 : INFO : topic #7 (0.111): 0.020*\"vape\" + 0.016*\"novape\" + 0.016*\"√∞¬æ√∞\" + 0.011*\"get\" + 0.011*\"√∞¬æ\" + 0.010*\"√∞¬µ√∞\" + 0.010*\"√∞¬µn\" + 0.010*\"√∞¬º√∞\" + 0.009*\"√∞¬µ\" + 0.008*\"√∞¬∫√∞\"\n",
      "2022-02-20 13:34:22,508 : INFO : topic #4 (0.111): 0.040*\"scary_antivape\" + 0.027*\"vape\" + 0.024*\"memes_tidepod\" + 0.021*\"nice_live\" + 0.021*\"bigchungus_spook\" + 0.019*\"antivape\" + 0.018*\"driving_spooky\" + 0.017*\"attention_january\" + 0.017*\"th_queenofengland\" + 0.008*\"get\"\n",
      "2022-02-20 13:34:22,509 : INFO : topic #1 (0.111): 0.034*\"vape\" + 0.016*\"nicotine\" + 0.011*\"know\" + 0.009*\"stopvape\" + 0.008*\"smoke\" + 0.008*\"life\" + 0.007*\"antismoking_antivape\" + 0.007*\"cigarette\" + 0.007*\"novape\" + 0.006*\"juul\"\n",
      "2022-02-20 13:34:22,510 : INFO : topic #3 (0.111): 0.045*\"novape\" + 0.017*\"nosmoke\" + 0.015*\"app_cegahnarkoba\" + 0.010*\"und\" + 0.010*\"antivape_rokokelektrik\" + 0.009*\"antivape\" + 0.008*\"vape\" + 0.007*\"merokok_napza\" + 0.007*\"pada\" + 0.007*\"lol\"\n",
      "2022-02-20 13:34:22,511 : INFO : topic #0 (0.111): 0.042*\"vape\" + 0.019*\"novape\" + 0.008*\"day\" + 0.006*\"student\" + 0.006*\"people\" + 0.006*\"cigarette\" + 0.006*\"stop\" + 0.006*\"smoke\" + 0.005*\"stopvape\" + 0.005*\"ita\"\n",
      "2022-02-20 13:34:22,512 : INFO : topic diff=0.509469, rho=0.577350\n",
      "2022-02-20 13:34:22,514 : INFO : PROGRESS: pass 0, at document #8000/8887\n",
      "2022-02-20 13:34:22,925 : INFO : merging changes from 2000 documents into a model of 8887 documents\n",
      "2022-02-20 13:34:22,934 : INFO : topic #7 (0.111): 0.017*\"vape\" + 0.012*\"get\" + 0.011*\"novape\" + 0.010*\"√∞¬æ√∞\" + 0.007*\"smoke\" + 0.006*\"√∞¬µ√∞\" + 0.006*\"√∞¬æ\" + 0.006*\"dontvape\" + 0.006*\"√∞¬µn\" + 0.006*\"√∞¬º√∞\"\n",
      "2022-02-20 13:34:22,935 : INFO : topic #1 (0.111): 0.035*\"vape\" + 0.018*\"nicotine\" + 0.010*\"know\" + 0.008*\"smoke\" + 0.008*\"get\" + 0.008*\"cigarette\" + 0.007*\"life\" + 0.006*\"juul\" + 0.006*\"vapelife\" + 0.006*\"good\"\n",
      "2022-02-20 13:34:22,936 : INFO : topic #0 (0.111): 0.044*\"vape\" + 0.014*\"novape\" + 0.007*\"people\" + 0.006*\"ita\" + 0.006*\"day\" + 0.006*\"cigarette\" + 0.005*\"stop\" + 0.005*\"danger\" + 0.005*\"student\" + 0.005*\"high\"\n",
      "2022-02-20 13:34:22,937 : INFO : topic #6 (0.111): 0.017*\"vape\" + 0.015*\"memes_tidepod\" + 0.014*\"get\" + 0.013*\"driving_spooky\" + 0.013*\"scary_antivape\" + 0.011*\"jimmyneutron_nfl\" + 0.011*\"memehouse\" + 0.011*\"world_carlwheezer\" + 0.011*\"ricardo_meandtheboy\" + 0.010*\"novape\"\n",
      "2022-02-20 13:34:22,938 : INFO : topic #5 (0.111): 0.032*\"novape\" + 0.024*\"nosmoke\" + 0.013*\"stopvaping\" + 0.013*\"smoke\" + 0.011*\"minecraft\" + 0.008*\"hookah_sg\" + 0.007*\"stopvape\" + 0.006*\"vape\" + 0.006*\"vapenation\" + 0.005*\"dontvape\"\n",
      "2022-02-20 13:34:22,939 : INFO : topic diff=0.466674, rho=0.500000\n",
      "2022-02-20 13:34:23,265 : INFO : -9.334 per-word bound, 645.3 perplexity estimate based on a held-out corpus of 887 documents with 23796 words\n",
      "2022-02-20 13:34:23,266 : INFO : PROGRESS: pass 0, at document #8887/8887\n",
      "2022-02-20 13:34:23,498 : INFO : merging changes from 887 documents into a model of 8887 documents\n",
      "2022-02-20 13:34:23,507 : INFO : topic #3 (0.111): 0.024*\"novape\" + 0.012*\"funny\" + 0.008*\"nosmoke\" + 0.007*\"philly\" + 0.007*\"da\" + 0.007*\"vape\" + 0.007*\"app_cegahnarkoba\" + 0.006*\"olympus_xa\" + 0.005*\"fuck\" + 0.005*\"battleroyale_minecraft\"\n",
      "2022-02-20 13:34:23,508 : INFO : topic #4 (0.111): 0.023*\"vape\" + 0.021*\"scary_antivape\" + 0.013*\"memes_tidepod\" + 0.012*\"meme\" + 0.012*\"day\" + 0.011*\"bigchungus_spook\" + 0.011*\"nice_live\" + 0.011*\"_\" + 0.010*\"driving_spooky\" + 0.009*\"antivape\"\n",
      "2022-02-20 13:34:23,509 : INFO : topic #1 (0.111): 0.035*\"vape\" + 0.021*\"nicotine\" + 0.009*\"know\" + 0.008*\"ita\" + 0.008*\"get\" + 0.007*\"life\" + 0.007*\"smoke\" + 0.007*\"year\" + 0.007*\"time\" + 0.007*\"good\"\n",
      "2022-02-20 13:34:23,510 : INFO : topic #2 (0.111): 0.049*\"quit\" + 0.043*\"vape\" + 0.019*\"help\" + 0.018*\"smoking\" + 0.015*\"health\" + 0.015*\"nicotine\" + 0.012*\"smoke\" + 0.012*\"quitsmoke\" + 0.011*\"cigarette\" + 0.011*\"quitvaping\"\n",
      "2022-02-20 13:34:23,512 : INFO : topic #0 (0.111): 0.042*\"vape\" + 0.009*\"people\" + 0.008*\"novape\" + 0.006*\"day\" + 0.006*\"danger\" + 0.006*\"center\" + 0.005*\"say\" + 0.005*\"cigarette\" + 0.005*\"death\" + 0.005*\"ita\"\n",
      "2022-02-20 13:34:23,512 : INFO : topic diff=0.548472, rho=0.447214\n",
      "2022-02-20 13:34:23,514 : INFO : LdaModel lifecycle event {'msg': 'trained LdaModel(num_terms=21122, num_topics=9, decay=0.5, chunksize=2000) in 2.57s', 'datetime': '2022-02-20T13:34:23.514312', 'gensim': '4.1.2', 'python': '3.8.11 (default, Aug  6 2021, 08:56:27) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.14.6-x86_64-i386-64bit', 'event': 'created'}\n",
      "2022-02-20 13:34:23,523 : INFO : LdaState lifecycle event {'fname_or_handle': '/Users/carol/opt/anaconda3/lib/python3.8/site-packages/gensim/test/test_data/model_9.state', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2022-02-20T13:34:23.523625', 'gensim': '4.1.2', 'python': '3.8.11 (default, Aug  6 2021, 08:56:27) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.14.6-x86_64-i386-64bit', 'event': 'saving'}\n",
      "2022-02-20 13:34:23,526 : INFO : saved /Users/carol/opt/anaconda3/lib/python3.8/site-packages/gensim/test/test_data/model_9.state\n",
      "2022-02-20 13:34:23,534 : INFO : LdaModel lifecycle event {'fname_or_handle': '/Users/carol/opt/anaconda3/lib/python3.8/site-packages/gensim/test/test_data/model_9', 'separately': \"['expElogbeta', 'sstats']\", 'sep_limit': 10485760, 'ignore': ['state', 'dispatcher', 'id2word'], 'datetime': '2022-02-20T13:34:23.534368', 'gensim': '4.1.2', 'python': '3.8.11 (default, Aug  6 2021, 08:56:27) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.14.6-x86_64-i386-64bit', 'event': 'saving'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-20 13:34:23,535 : INFO : storing np array 'expElogbeta' to /Users/carol/opt/anaconda3/lib/python3.8/site-packages/gensim/test/test_data/model_9.expElogbeta.npy\n",
      "2022-02-20 13:34:23,537 : INFO : not storing attribute state\n",
      "2022-02-20 13:34:23,538 : INFO : not storing attribute dispatcher\n",
      "2022-02-20 13:34:23,539 : INFO : not storing attribute id2word\n",
      "2022-02-20 13:34:23,543 : INFO : saved /Users/carol/opt/anaconda3/lib/python3.8/site-packages/gensim/test/test_data/model_9\n",
      "2022-02-20 13:34:23,548 : INFO : using ParallelWordOccurrenceAccumulator(processes=7, batch_size=64) to estimate probabilities from sliding windows\n",
      "2022-02-20 13:34:28,824 : INFO : 7 accumulators retrieved from output queue\n",
      "2022-02-20 13:34:28,850 : INFO : accumulated word occurrence stats for 9636 virtual documents\n",
      "2022-02-20 13:34:29,162 : INFO : using symmetric alpha at 0.1\n",
      "2022-02-20 13:34:29,163 : INFO : using symmetric eta at 0.1\n",
      "2022-02-20 13:34:29,166 : INFO : using serial LDA version on this node\n",
      "2022-02-20 13:34:29,187 : INFO : running online (single-pass) LDA training, 10 topics, 1 passes over the supplied corpus of 8887 documents, updating model once every 2000 documents, evaluating perplexity every 8887 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2022-02-20 13:34:29,188 : WARNING : too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      "2022-02-20 13:34:29,190 : INFO : PROGRESS: pass 0, at document #2000/8887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of topic is:  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-20 13:34:29,852 : INFO : merging changes from 2000 documents into a model of 8887 documents\n",
      "2022-02-20 13:34:29,861 : INFO : topic #4 (0.100): 0.036*\"vape\" + 0.014*\"stopvaping\" + 0.014*\"stopvape\" + 0.013*\"stopsmoke\" + 0.009*\"smoking\" + 0.009*\"cigarette\" + 0.008*\"quit\" + 0.007*\"novape\" + 0.007*\"tobacco\" + 0.006*\"stop\"\n",
      "2022-02-20 13:34:29,863 : INFO : topic #6 (0.100): 0.027*\"vape\" + 0.014*\"smoke\" + 0.014*\"stopvaping\" + 0.013*\"stopvape\" + 0.012*\"novape\" + 0.011*\"cigarette\" + 0.009*\"know\" + 0.006*\"stop\" + 0.005*\"product\" + 0.005*\"make\"\n",
      "2022-02-20 13:34:29,864 : INFO : topic #9 (0.100): 0.034*\"novape\" + 0.019*\"vape\" + 0.015*\"nosmoke\" + 0.012*\"smoke\" + 0.012*\"stopvaping\" + 0.005*\"health\" + 0.005*\"kid\" + 0.005*\"sign\" + 0.005*\"know\" + 0.004*\"stopvape\"\n",
      "2022-02-20 13:34:29,865 : INFO : topic #8 (0.100): 0.028*\"stopvaping\" + 0.020*\"vape\" + 0.010*\"novape\" + 0.010*\"dm_lingle\" + 0.010*\"oml\" + 0.009*\"soon_toosoon\" + 0.008*\"stopsmoke\" + 0.007*\"stop\" + 0.007*\"nosmoke\" + 0.007*\"get\"\n",
      "2022-02-20 13:34:29,866 : INFO : topic #5 (0.100): 0.042*\"vape\" + 0.018*\"novape\" + 0.012*\"√∞¬æ√∞\" + 0.012*\"√∞¬æ\" + 0.010*\"stopvape\" + 0.008*\"√∞¬º√∞\" + 0.007*\"√∞¬µn\" + 0.007*\"√∞¬µ√∞\" + 0.007*\"stop\" + 0.007*\"people\"\n",
      "2022-02-20 13:34:29,867 : INFO : topic diff=8.740729, rho=1.000000\n",
      "2022-02-20 13:34:29,869 : INFO : PROGRESS: pass 0, at document #4000/8887\n",
      "2022-02-20 13:34:30,319 : INFO : merging changes from 2000 documents into a model of 8887 documents\n",
      "2022-02-20 13:34:30,329 : INFO : topic #0 (0.100): 0.034*\"novape\" + 0.017*\"nosmoke\" + 0.015*\"smoke\" + 0.013*\"day\" + 0.012*\"stopvaping\" + 0.010*\"vape\" + 0.008*\"quit\" + 0.008*\"nicotine\" + 0.007*\"tobacco\" + 0.006*\"get\"\n",
      "2022-02-20 13:34:30,329 : INFO : topic #2 (0.100): 0.053*\"novape\" + 0.038*\"vape\" + 0.020*\"nosmoke\" + 0.019*\"stopvape\" + 0.010*\"smoke\" + 0.009*\"smoking\" + 0.009*\"cigarette\" + 0.008*\"lung\" + 0.007*\"nicotine\" + 0.006*\"tobacco\"\n",
      "2022-02-20 13:34:30,331 : INFO : topic #6 (0.100): 0.032*\"vape\" + 0.015*\"cigarette\" + 0.014*\"smoke\" + 0.012*\"novape\" + 0.012*\"stopvape\" + 0.011*\"stopvaping\" + 0.011*\"know\" + 0.007*\"stop\" + 0.007*\"product\" + 0.006*\"quit\"\n",
      "2022-02-20 13:34:30,331 : INFO : topic #7 (0.100): 0.067*\"novape\" + 0.039*\"nosmoke\" + 0.027*\"vape\" + 0.016*\"smoke\" + 0.010*\"stopvape\" + 0.008*\"sign\" + 0.007*\"quit\" + 0.007*\"smoking\" + 0.006*\"know\" + 0.006*\"cigarette\"\n",
      "2022-02-20 13:34:30,333 : INFO : topic #1 (0.100): 0.053*\"vape\" + 0.016*\"novape\" + 0.015*\"cigarette\" + 0.013*\"stopvaping\" + 0.010*\"stopvape\" + 0.009*\"smoke\" + 0.009*\"get\" + 0.009*\"health\" + 0.007*\"tobacco\" + 0.007*\"student\"\n",
      "2022-02-20 13:34:30,334 : INFO : topic diff=0.399694, rho=0.707107\n",
      "2022-02-20 13:34:30,335 : INFO : PROGRESS: pass 0, at document #6000/8887\n",
      "2022-02-20 13:34:30,702 : INFO : merging changes from 2000 documents into a model of 8887 documents\n",
      "2022-02-20 13:34:30,711 : INFO : topic #7 (0.100): 0.053*\"novape\" + 0.041*\"nosmoke\" + 0.021*\"vape\" + 0.015*\"smoke\" + 0.011*\"quit\" + 0.009*\"minecraft\" + 0.008*\"scary_antivape\" + 0.007*\"memes_tidepod\" + 0.007*\"driving_spooky\" + 0.007*\"school\"\n",
      "2022-02-20 13:34:30,713 : INFO : topic #8 (0.100): 0.026*\"stopvaping\" + 0.018*\"app_cegahnarkoba\" + 0.015*\"oml\" + 0.014*\"dm_lingle\" + 0.013*\"soon_toosoon\" + 0.011*\"antivape_rokokelektrik\" + 0.010*\"dengan\" + 0.010*\"vape\" + 0.007*\"merokok_napza\" + 0.007*\"stoprokok_rokok\"\n",
      "2022-02-20 13:34:30,714 : INFO : topic #3 (0.100): 0.015*\"nicotine\" + 0.015*\"smoke\" + 0.013*\"day\" + 0.013*\"quit\" + 0.013*\"get\" + 0.012*\"vape\" + 0.009*\"go\" + 0.009*\"today\" + 0.009*\"time\" + 0.009*\"stopvape\"\n",
      "2022-02-20 13:34:30,715 : INFO : topic #0 (0.100): 0.041*\"driving_spooky\" + 0.040*\"scary_antivape\" + 0.040*\"memes_tidepod\" + 0.027*\"world_carlwheezer\" + 0.027*\"jimmyneutron_nfl\" + 0.027*\"memehouse\" + 0.027*\"ricardo_meandtheboy\" + 0.018*\"novape\" + 0.016*\"day\" + 0.010*\"nosmoke\"\n",
      "2022-02-20 13:34:30,716 : INFO : topic #1 (0.100): 0.052*\"vape\" + 0.012*\"cigarette\" + 0.012*\"novape\" + 0.011*\"health\" + 0.010*\"get\" + 0.008*\"smoke\" + 0.008*\"student\" + 0.007*\"stopvaping\" + 0.007*\"follow\" + 0.006*\"tobacco\"\n",
      "2022-02-20 13:34:30,717 : INFO : topic diff=0.421803, rho=0.577350\n",
      "2022-02-20 13:34:30,718 : INFO : PROGRESS: pass 0, at document #8000/8887\n",
      "2022-02-20 13:34:31,125 : INFO : merging changes from 2000 documents into a model of 8887 documents\n",
      "2022-02-20 13:34:31,134 : INFO : topic #7 (0.100): 0.039*\"novape\" + 0.031*\"nosmoke\" + 0.026*\"dontvape\" + 0.019*\"vape\" + 0.014*\"smoke\" + 0.011*\"meme\" + 0.009*\"quit\" + 0.008*\"funny\" + 0.008*\"minecraft\" + 0.007*\"school\"\n",
      "2022-02-20 13:34:31,135 : INFO : topic #8 (0.100): 0.017*\"stopvaping\" + 0.017*\"app_cegahnarkoba\" + 0.009*\"antivape_rokokelektrik\" + 0.009*\"oml\" + 0.008*\"dm_lingle\" + 0.008*\"dengan\" + 0.008*\"soon_toosoon\" + 0.007*\"vape\" + 0.007*\"art\" + 0.006*\"merokok_napza\"\n",
      "2022-02-20 13:34:31,136 : INFO : topic #6 (0.100): 0.029*\"vape\" + 0.016*\"cigarette\" + 0.014*\"smoke\" + 0.012*\"know\" + 0.011*\"quit\" + 0.009*\"antivape\" + 0.008*\"smoking\" + 0.008*\"product\" + 0.008*\"nicotine\" + 0.007*\"people\"\n",
      "2022-02-20 13:34:31,136 : INFO : topic #4 (0.100): 0.038*\"vape\" + 0.035*\"dontvape\" + 0.026*\"quit\" + 0.020*\"antivape\" + 0.014*\"smoking\" + 0.012*\"cigarette\" + 0.012*\"nicotine\" + 0.011*\"tobacco\" + 0.010*\"health\" + 0.010*\"juul\"\n",
      "2022-02-20 13:34:31,137 : INFO : topic #9 (0.100): 0.034*\"vape\" + 0.023*\"scary_antivape\" + 0.022*\"bigchungus_spook\" + 0.022*\"nice_live\" + 0.018*\"attention_january\" + 0.018*\"th_queenofengland\" + 0.015*\"novape\" + 0.014*\"nice\" + 0.010*\"carlwheezer_jimmyneutron\" + 0.010*\"meandtheboys_memehouse\"\n",
      "2022-02-20 13:34:31,138 : INFO : topic diff=0.364155, rho=0.500000\n",
      "2022-02-20 13:34:31,443 : INFO : -9.603 per-word bound, 777.5 perplexity estimate based on a held-out corpus of 887 documents with 23796 words\n",
      "2022-02-20 13:34:31,444 : INFO : PROGRESS: pass 0, at document #8887/8887\n",
      "2022-02-20 13:34:31,665 : INFO : merging changes from 887 documents into a model of 8887 documents\n",
      "2022-02-20 13:34:31,674 : INFO : topic #9 (0.100): 0.028*\"vape\" + 0.015*\"scary_antivape\" + 0.014*\"bigchungus_spook\" + 0.014*\"nice_live\" + 0.013*\"_\" + 0.012*\"photo\" + 0.012*\"attention_january\" + 0.012*\"th_queenofengland\" + 0.012*\"novape\" + 0.010*\"nice\"\n",
      "2022-02-20 13:34:31,675 : INFO : topic #6 (0.100): 0.030*\"vape\" + 0.017*\"cigarette\" + 0.013*\"smoke\" + 0.012*\"quit\" + 0.012*\"know\" + 0.010*\"product\" + 0.010*\"nicotine\" + 0.008*\"smoking\" + 0.007*\"flavor\" + 0.007*\"people\"\n",
      "2022-02-20 13:34:31,676 : INFO : topic #8 (0.100): 0.012*\"stopvaping\" + 0.010*\"app_cegahnarkoba\" + 0.009*\"lmao\" + 0.009*\"draw\" + 0.008*\"wellbee\" + 0.007*\"meme\" + 0.007*\"tired\" + 0.006*\"art\" + 0.006*\"furry_autistic\" + 0.006*\"kek_triggere\"\n",
      "2022-02-20 13:34:31,677 : INFO : topic #4 (0.100): 0.040*\"vape\" + 0.038*\"quit\" + 0.019*\"smoking\" + 0.018*\"help\" + 0.018*\"nicotine\" + 0.015*\"stopsmoke\" + 0.015*\"quitsmoke\" + 0.014*\"quitvaping\" + 0.013*\"health\" + 0.013*\"quitsmoking\"\n",
      "2022-02-20 13:34:31,679 : INFO : topic #7 (0.100): 0.026*\"novape\" + 0.022*\"nosmoke\" + 0.016*\"vape\" + 0.016*\"dontvape\" + 0.012*\"smoke\" + 0.009*\"quit\" + 0.009*\"funny\" + 0.008*\"nicotinefree\" + 0.008*\"meme\" + 0.007*\"finally\"\n",
      "2022-02-20 13:34:31,680 : INFO : topic diff=0.473972, rho=0.447214\n",
      "2022-02-20 13:34:31,681 : INFO : LdaModel lifecycle event {'msg': 'trained LdaModel(num_terms=21122, num_topics=10, decay=0.5, chunksize=2000) in 2.49s', 'datetime': '2022-02-20T13:34:31.681151', 'gensim': '4.1.2', 'python': '3.8.11 (default, Aug  6 2021, 08:56:27) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.14.6-x86_64-i386-64bit', 'event': 'created'}\n",
      "2022-02-20 13:34:31,687 : INFO : LdaState lifecycle event {'fname_or_handle': '/Users/carol/opt/anaconda3/lib/python3.8/site-packages/gensim/test/test_data/model_10.state', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2022-02-20T13:34:31.687353', 'gensim': '4.1.2', 'python': '3.8.11 (default, Aug  6 2021, 08:56:27) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.14.6-x86_64-i386-64bit', 'event': 'saving'}\n",
      "2022-02-20 13:34:31,689 : INFO : saved /Users/carol/opt/anaconda3/lib/python3.8/site-packages/gensim/test/test_data/model_10.state\n",
      "2022-02-20 13:34:31,701 : INFO : LdaModel lifecycle event {'fname_or_handle': '/Users/carol/opt/anaconda3/lib/python3.8/site-packages/gensim/test/test_data/model_10', 'separately': \"['expElogbeta', 'sstats']\", 'sep_limit': 10485760, 'ignore': ['state', 'dispatcher', 'id2word'], 'datetime': '2022-02-20T13:34:31.701167', 'gensim': '4.1.2', 'python': '3.8.11 (default, Aug  6 2021, 08:56:27) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.14.6-x86_64-i386-64bit', 'event': 'saving'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-20 13:34:31,702 : INFO : storing np array 'expElogbeta' to /Users/carol/opt/anaconda3/lib/python3.8/site-packages/gensim/test/test_data/model_10.expElogbeta.npy\n",
      "2022-02-20 13:34:31,705 : INFO : not storing attribute state\n",
      "2022-02-20 13:34:31,706 : INFO : not storing attribute dispatcher\n",
      "2022-02-20 13:34:31,708 : INFO : not storing attribute id2word\n",
      "2022-02-20 13:34:31,711 : INFO : saved /Users/carol/opt/anaconda3/lib/python3.8/site-packages/gensim/test/test_data/model_10\n",
      "2022-02-20 13:34:31,716 : INFO : using ParallelWordOccurrenceAccumulator(processes=7, batch_size=64) to estimate probabilities from sliding windows\n",
      "2022-02-20 13:34:37,766 : INFO : 7 accumulators retrieved from output queue\n",
      "2022-02-20 13:34:37,795 : INFO : accumulated word occurrence stats for 9655 virtual documents\n",
      "2022-02-20 13:34:38,131 : INFO : using symmetric alpha at 0.09090909090909091\n",
      "2022-02-20 13:34:38,132 : INFO : using symmetric eta at 0.09090909090909091\n",
      "2022-02-20 13:34:38,135 : INFO : using serial LDA version on this node\n",
      "2022-02-20 13:34:38,160 : INFO : running online (single-pass) LDA training, 11 topics, 1 passes over the supplied corpus of 8887 documents, updating model once every 2000 documents, evaluating perplexity every 8887 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2022-02-20 13:34:38,160 : WARNING : too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      "2022-02-20 13:34:38,162 : INFO : PROGRESS: pass 0, at document #2000/8887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of topic is:  11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-20 13:34:38,838 : INFO : merging changes from 2000 documents into a model of 8887 documents\n",
      "2022-02-20 13:34:38,849 : INFO : topic #3 (0.091): 0.020*\"novape\" + 0.018*\"vape\" + 0.008*\"stopvape\" + 0.008*\"time\" + 0.007*\"stopvaping\" + 0.007*\"help\" + 0.006*\"quit\" + 0.006*\"get\" + 0.006*\"sign\" + 0.006*\"cigarette\"\n",
      "2022-02-20 13:34:38,850 : INFO : topic #10 (0.091): 0.033*\"novape\" + 0.033*\"vape\" + 0.019*\"stopvaping\" + 0.010*\"stopvape\" + 0.006*\"free\" + 0.006*\"know\" + 0.006*\"health\" + 0.005*\"stopsmoke\" + 0.005*\"kid\" + 0.004*\"make\"\n",
      "2022-02-20 13:34:38,851 : INFO : topic #8 (0.091): 0.044*\"vape\" + 0.018*\"novape\" + 0.016*\"stopvape\" + 0.014*\"nosmoke\" + 0.014*\"cigarette\" + 0.014*\"smoke\" + 0.010*\"stopvaping\" + 0.007*\"nicotine\" + 0.006*\"get\" + 0.006*\"people\"\n",
      "2022-02-20 13:34:38,852 : INFO : topic #1 (0.091): 0.028*\"vape\" + 0.017*\"novape\" + 0.014*\"stopvaping\" + 0.009*\"√∞¬µ√∞\" + 0.007*\"stopvape\" + 0.007*\"√∞¬æ\" + 0.006*\"smoke\" + 0.006*\"smokefree\" + 0.006*\"take\" + 0.006*\"vapefree\"\n",
      "2022-02-20 13:34:38,853 : INFO : topic #5 (0.091): 0.027*\"novape\" + 0.013*\"nosmoke\" + 0.011*\"vape\" + 0.009*\"stopvape\" + 0.009*\"√∞¬º√∞\" + 0.007*\"stopvaping\" + 0.007*\"stopsmoke\" + 0.005*\"√∞¬µn\" + 0.005*\"cigarette\" + 0.005*\"√∞¬æ√∞\"\n",
      "2022-02-20 13:34:38,854 : INFO : topic diff=9.552016, rho=1.000000\n",
      "2022-02-20 13:34:38,856 : INFO : PROGRESS: pass 0, at document #4000/8887\n",
      "2022-02-20 13:34:39,299 : INFO : merging changes from 2000 documents into a model of 8887 documents\n",
      "2022-02-20 13:34:39,309 : INFO : topic #0 (0.091): 0.042*\"novape\" + 0.034*\"vape\" + 0.022*\"nosmoke\" + 0.022*\"smoke\" + 0.013*\"quit\" + 0.012*\"smoking\" + 0.011*\"stopvape\" + 0.010*\"get\" + 0.009*\"stopvaping\" + 0.009*\"stop\"\n",
      "2022-02-20 13:34:39,310 : INFO : topic #1 (0.091): 0.025*\"vape\" + 0.017*\"novape\" + 0.014*\"√∞¬µ√∞\" + 0.013*\"√∞¬æ√∞\" + 0.012*\"√∞¬æ\" + 0.011*\"√∞¬µn\" + 0.010*\"stopvaping\" + 0.009*\"vapefree\" + 0.009*\"√∞¬∫√∞\" + 0.008*\"smokefree\"\n",
      "2022-02-20 13:34:39,311 : INFO : topic #9 (0.091): 0.020*\"novape\" + 0.018*\"stopvaping\" + 0.017*\"√∞¬æ√∞\" + 0.011*\"vape\" + 0.008*\"nosmoke\" + 0.007*\"√∞¬µ\" + 0.007*\"√∞¬º√∞\" + 0.006*\"stopvape\" + 0.006*\"day\" + 0.006*\"nanostick_stopsmoke\"\n",
      "2022-02-20 13:34:39,312 : INFO : topic #10 (0.091): 0.042*\"novape\" + 0.033*\"vape\" + 0.031*\"stopvaping\" + 0.012*\"dm_lingle\" + 0.012*\"oml\" + 0.012*\"soon_toosoon\" + 0.008*\"stopvape\" + 0.008*\"antivape\" + 0.005*\"know\" + 0.005*\"minecraft\"\n",
      "2022-02-20 13:34:39,313 : INFO : topic #7 (0.091): 0.039*\"vape\" + 0.024*\"stopvape\" + 0.020*\"nicotine\" + 0.012*\"novape\" + 0.012*\"cigarette\" + 0.010*\"stopvaping\" + 0.009*\"health\" + 0.008*\"juul\" + 0.007*\"risk\" + 0.006*\"teen\"\n",
      "2022-02-20 13:34:39,314 : INFO : topic diff=0.415410, rho=0.707107\n",
      "2022-02-20 13:34:39,316 : INFO : PROGRESS: pass 0, at document #6000/8887\n",
      "2022-02-20 13:34:39,671 : INFO : merging changes from 2000 documents into a model of 8887 documents\n",
      "2022-02-20 13:34:39,681 : INFO : topic #1 (0.091): 0.021*\"vape\" + 0.014*\"novape\" + 0.012*\"√∞¬µ√∞\" + 0.010*\"√∞¬æ√∞\" + 0.009*\"√∞¬æ\" + 0.009*\"√∞¬µn\" + 0.008*\"√∞¬∫√∞\" + 0.007*\"smokefree\" + 0.007*\"vapefree\" + 0.007*\"get\"\n",
      "2022-02-20 13:34:39,682 : INFO : topic #5 (0.091): 0.021*\"novape\" + 0.013*\"√∞¬º√∞\" + 0.012*\"nosmoke\" + 0.008*\"vape\" + 0.007*\"√∞¬µn\" + 0.007*\"meme\" + 0.006*\"membantu\" + 0.006*\"√∞¬µ\" + 0.006*\"√∞¬æ√∞\" + 0.005*\"lol\"\n",
      "2022-02-20 13:34:39,683 : INFO : topic #3 (0.091): 0.026*\"vape\" + 0.023*\"nice\" + 0.016*\"novape\" + 0.013*\"pumpkinspice_antivape\" + 0.013*\"driving_nonutnovember\" + 0.013*\"changedotorg_world\" + 0.013*\"meandtheboys_memehouse\" + 0.013*\"carlwheezer_jimmyneutron\" + 0.009*\"time\" + 0.007*\"get\"\n",
      "2022-02-20 13:34:39,685 : INFO : topic #4 (0.091): 0.015*\"und\" + 0.009*\"whatsapp\" + 0.009*\"novape\" + 0.007*\"make\" + 0.006*\"get\" + 0.006*\"smoke\" + 0.006*\"stopvaping\" + 0.005*\"repost\" + 0.005*\"tobacco\" + 0.005*\"vapeporn\"\n",
      "2022-02-20 13:34:39,686 : INFO : topic #7 (0.091): 0.037*\"vape\" + 0.031*\"nicotine\" + 0.014*\"stopvape\" + 0.009*\"juul\" + 0.009*\"novape\" + 0.008*\"cigarette\" + 0.007*\"health\" + 0.007*\"antivape\" + 0.006*\"vapenation\" + 0.006*\"stopvaping\"\n",
      "2022-02-20 13:34:39,686 : INFO : topic diff=0.371618, rho=0.577350\n",
      "2022-02-20 13:34:39,688 : INFO : PROGRESS: pass 0, at document #8000/8887\n",
      "2022-02-20 13:34:40,068 : INFO : merging changes from 2000 documents into a model of 8887 documents\n",
      "2022-02-20 13:34:40,078 : INFO : topic #6 (0.091): 0.020*\"dontvape\" + 0.014*\"cloud\" + 0.011*\"dontsmoke\" + 0.010*\"novape\" + 0.009*\"vape\" + 0.008*\"dog\" + 0.008*\"light\" + 0.008*\"practice\" + 0.007*\"vapelife\" + 0.006*\"safety\"\n",
      "2022-02-20 13:34:40,079 : INFO : topic #7 (0.091): 0.039*\"vape\" + 0.035*\"nicotine\" + 0.017*\"dontvape\" + 0.014*\"juul\" + 0.010*\"stopvape\" + 0.009*\"cigarette\" + 0.008*\"teen\" + 0.007*\"vapenation\" + 0.006*\"risk\" + 0.006*\"novape\"\n",
      "2022-02-20 13:34:40,080 : INFO : topic #9 (0.091): 0.064*\"scary_antivape\" + 0.055*\"memes_tidepod\" + 0.050*\"driving_spooky\" + 0.021*\"meme\" + 0.016*\"day\" + 0.015*\"world_carlwheezer\" + 0.015*\"ricardo_meandtheboy\" + 0.015*\"memehouse\" + 0.015*\"jimmyneutron_nfl\" + 0.013*\"follow\"\n",
      "2022-02-20 13:34:40,081 : INFO : topic #3 (0.091): 0.031*\"vape\" + 0.023*\"nice\" + 0.016*\"pumpkinspice_antivape\" + 0.016*\"driving_nonutnovember\" + 0.016*\"changedotorg_world\" + 0.016*\"meandtheboys_memehouse\" + 0.016*\"carlwheezer_jimmyneutron\" + 0.014*\"novape\" + 0.008*\"time\" + 0.006*\"get\"\n",
      "2022-02-20 13:34:40,082 : INFO : topic #10 (0.091): 0.043*\"dontvape\" + 0.026*\"antivape\" + 0.026*\"vape\" + 0.019*\"novape\" + 0.015*\"minecraft\" + 0.011*\"stopvaping\" + 0.009*\"hookah_sg\" + 0.006*\"coffee\" + 0.006*\"juulnation\" + 0.006*\"good\"\n",
      "2022-02-20 13:34:40,083 : INFO : topic diff=0.280469, rho=0.500000\n",
      "2022-02-20 13:34:40,385 : INFO : -9.996 per-word bound, 1020.9 perplexity estimate based on a held-out corpus of 887 documents with 23796 words\n",
      "2022-02-20 13:34:40,386 : INFO : PROGRESS: pass 0, at document #8887/8887\n",
      "2022-02-20 13:34:40,605 : INFO : merging changes from 887 documents into a model of 8887 documents\n",
      "2022-02-20 13:34:40,614 : INFO : topic #6 (0.091): 0.016*\"music\" + 0.012*\"dontvape\" + 0.009*\"play\" + 0.009*\"vapelife\" + 0.008*\"cloud\" + 0.007*\"suppose\" + 0.007*\"dance\" + 0.007*\"safety\" + 0.007*\"dontsmoke\" + 0.007*\"vape\"\n",
      "2022-02-20 13:34:40,616 : INFO : topic #7 (0.091): 0.050*\"nicotine\" + 0.038*\"vape\" + 0.017*\"juul\" + 0.010*\"cigarette\" + 0.010*\"dontvape\" + 0.009*\"stopvape\" + 0.008*\"teen\" + 0.008*\"level\" + 0.007*\"pod\" + 0.007*\"vapenation\"\n",
      "2022-02-20 13:34:40,617 : INFO : topic #4 (0.091): 0.016*\"_\" + 0.014*\"stress\" + 0.010*\"beauty\" + 0.009*\"forever\" + 0.008*\"effort\" + 0.007*\"hypnosis\" + 0.007*\"und\" + 0.006*\"image\" + 0.005*\"tired\" + 0.005*\"nojuuljuly\"\n",
      "2022-02-20 13:34:40,618 : INFO : topic #9 (0.091): 0.045*\"scary_antivape\" + 0.039*\"memes_tidepod\" + 0.036*\"driving_spooky\" + 0.019*\"meme\" + 0.015*\"funny\" + 0.014*\"day\" + 0.011*\"vape\" + 0.010*\"follow\" + 0.010*\"jimmyneutron_nfl\" + 0.010*\"memehouse\"\n",
      "2022-02-20 13:34:40,619 : INFO : topic #8 (0.091): 0.059*\"vape\" + 0.020*\"cigarette\" + 0.018*\"nicotine\" + 0.012*\"health\" + 0.011*\"quitvape\" + 0.010*\"tobacco\" + 0.009*\"smoke\" + 0.008*\"product\" + 0.008*\"quit\" + 0.007*\"help\"\n",
      "2022-02-20 13:34:40,620 : INFO : topic diff=0.297967, rho=0.447214\n",
      "2022-02-20 13:34:40,622 : INFO : LdaModel lifecycle event {'msg': 'trained LdaModel(num_terms=21122, num_topics=11, decay=0.5, chunksize=2000) in 2.46s', 'datetime': '2022-02-20T13:34:40.622035', 'gensim': '4.1.2', 'python': '3.8.11 (default, Aug  6 2021, 08:56:27) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.14.6-x86_64-i386-64bit', 'event': 'created'}\n",
      "2022-02-20 13:34:40,631 : INFO : LdaState lifecycle event {'fname_or_handle': '/Users/carol/opt/anaconda3/lib/python3.8/site-packages/gensim/test/test_data/model_11.state', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2022-02-20T13:34:40.631107', 'gensim': '4.1.2', 'python': '3.8.11 (default, Aug  6 2021, 08:56:27) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.14.6-x86_64-i386-64bit', 'event': 'saving'}\n",
      "2022-02-20 13:34:40,633 : INFO : saved /Users/carol/opt/anaconda3/lib/python3.8/site-packages/gensim/test/test_data/model_11.state\n",
      "2022-02-20 13:34:40,643 : INFO : LdaModel lifecycle event {'fname_or_handle': '/Users/carol/opt/anaconda3/lib/python3.8/site-packages/gensim/test/test_data/model_11', 'separately': \"['expElogbeta', 'sstats']\", 'sep_limit': 10485760, 'ignore': ['state', 'dispatcher', 'id2word'], 'datetime': '2022-02-20T13:34:40.643803', 'gensim': '4.1.2', 'python': '3.8.11 (default, Aug  6 2021, 08:56:27) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.14.6-x86_64-i386-64bit', 'event': 'saving'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-20 13:34:40,644 : INFO : storing np array 'expElogbeta' to /Users/carol/opt/anaconda3/lib/python3.8/site-packages/gensim/test/test_data/model_11.expElogbeta.npy\n",
      "2022-02-20 13:34:40,648 : INFO : not storing attribute state\n",
      "2022-02-20 13:34:40,649 : INFO : not storing attribute dispatcher\n",
      "2022-02-20 13:34:40,650 : INFO : not storing attribute id2word\n",
      "2022-02-20 13:34:40,652 : INFO : saved /Users/carol/opt/anaconda3/lib/python3.8/site-packages/gensim/test/test_data/model_11\n",
      "2022-02-20 13:34:40,657 : INFO : using ParallelWordOccurrenceAccumulator(processes=7, batch_size=64) to estimate probabilities from sliding windows\n",
      "2022-02-20 13:34:45,613 : INFO : 7 accumulators retrieved from output queue\n",
      "2022-02-20 13:34:45,655 : INFO : accumulated word occurrence stats for 9630 virtual documents\n",
      "2022-02-20 13:34:46,082 : INFO : using symmetric alpha at 0.08333333333333333\n",
      "2022-02-20 13:34:46,083 : INFO : using symmetric eta at 0.08333333333333333\n",
      "2022-02-20 13:34:46,087 : INFO : using serial LDA version on this node\n",
      "2022-02-20 13:34:46,109 : INFO : running online (single-pass) LDA training, 12 topics, 1 passes over the supplied corpus of 8887 documents, updating model once every 2000 documents, evaluating perplexity every 8887 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2022-02-20 13:34:46,110 : WARNING : too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      "2022-02-20 13:34:46,112 : INFO : PROGRESS: pass 0, at document #2000/8887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of topic is:  12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-20 13:34:46,826 : INFO : merging changes from 2000 documents into a model of 8887 documents\n",
      "2022-02-20 13:34:46,837 : INFO : topic #10 (0.083): 0.054*\"novape\" + 0.020*\"nosmoke\" + 0.018*\"vape\" + 0.009*\"tobacco\" + 0.009*\"stopvape\" + 0.007*\"stopvaping\" + 0.007*\"cigarette\" + 0.005*\"smoke\" + 0.005*\"get\" + 0.005*\"time\"\n",
      "2022-02-20 13:34:46,838 : INFO : topic #9 (0.083): 0.020*\"novape\" + 0.014*\"stopvaping\" + 0.012*\"stopsmoke\" + 0.011*\"vape\" + 0.010*\"stopvape\" + 0.009*\"nosmoke\" + 0.008*\"quit\" + 0.006*\"know\" + 0.006*\"smoke\" + 0.006*\"tobacco\"\n",
      "2022-02-20 13:34:46,839 : INFO : topic #11 (0.083): 0.034*\"novape\" + 0.011*\"vape\" + 0.010*\"stopvape\" + 0.008*\"stopsmoke\" + 0.008*\"stop\" + 0.007*\"stopvaping\" + 0.007*\"nosmoke\" + 0.006*\"smoke\" + 0.006*\"day\" + 0.005*\"smoking\"\n",
      "2022-02-20 13:34:46,840 : INFO : topic #8 (0.083): 0.021*\"novape\" + 0.015*\"nosmoke\" + 0.009*\"vape\" + 0.007*\"come\" + 0.007*\"stopvaping\" + 0.006*\"nicotine\" + 0.005*\"stopvape\" + 0.005*\"cigarette\" + 0.004*\"teen\" + 0.004*\"stopsmoke\"\n",
      "2022-02-20 13:34:46,841 : INFO : topic #3 (0.083): 0.033*\"vape\" + 0.017*\"novape\" + 0.010*\"nosmoke\" + 0.010*\"cigarette\" + 0.009*\"stopvaping\" + 0.008*\"stopvape\" + 0.007*\"make\" + 0.006*\"take\" + 0.006*\"know\" + 0.005*\"√∞¬µ√∞\"\n",
      "2022-02-20 13:34:46,842 : INFO : topic diff=10.448144, rho=1.000000\n",
      "2022-02-20 13:34:46,844 : INFO : PROGRESS: pass 0, at document #4000/8887\n",
      "2022-02-20 13:34:47,454 : INFO : merging changes from 2000 documents into a model of 8887 documents\n",
      "2022-02-20 13:34:47,465 : INFO : topic #4 (0.083): 0.027*\"cigarette\" + 0.023*\"vape\" + 0.020*\"nicotine\" + 0.017*\"stopvape\" + 0.015*\"stopvaping\" + 0.012*\"teen\" + 0.011*\"know\" + 0.011*\"juul\" + 0.010*\"smoke\" + 0.008*\"tobacco\"\n",
      "2022-02-20 13:34:47,466 : INFO : topic #10 (0.083): 0.091*\"novape\" + 0.039*\"nosmoke\" + 0.020*\"vape\" + 0.015*\"antivape\" + 0.010*\"tobacco\" + 0.009*\"smokefree\" + 0.008*\"time\" + 0.007*\"sign\" + 0.007*\"vapefree\" + 0.006*\"get\"\n",
      "2022-02-20 13:34:47,467 : INFO : topic #6 (0.083): 0.035*\"vape\" + 0.014*\"stopvape\" + 0.013*\"help\" + 0.013*\"novape\" + 0.010*\"cigarette\" + 0.008*\"smoke\" + 0.008*\"stopvaping\" + 0.008*\"stopsmoke\" + 0.008*\"stop\" + 0.007*\"need\"\n",
      "2022-02-20 13:34:47,468 : INFO : topic #5 (0.083): 0.026*\"novape\" + 0.014*\"stopvaping\" + 0.011*\"stopvape\" + 0.010*\"nosmoke\" + 0.009*\"cigarette\" + 0.009*\"stopsmoke\" + 0.009*\"vape\" + 0.008*\"time\" + 0.006*\"day\" + 0.005*\"get\"\n",
      "2022-02-20 13:34:47,469 : INFO : topic #11 (0.083): 0.037*\"novape\" + 0.010*\"stopvape\" + 0.010*\"√∞¬∫√∞\" + 0.010*\"vape\" + 0.009*\"meme\" + 0.009*\"nosmoke\" + 0.008*\"stop\" + 0.008*\"stopsmoke\" + 0.008*\"lol\" + 0.007*\"√∞¬µ\"\n",
      "2022-02-20 13:34:47,471 : INFO : topic diff=0.349976, rho=0.707107\n",
      "2022-02-20 13:34:47,472 : INFO : PROGRESS: pass 0, at document #6000/8887\n",
      "2022-02-20 13:34:47,851 : INFO : merging changes from 2000 documents into a model of 8887 documents\n",
      "2022-02-20 13:34:47,862 : INFO : topic #2 (0.083): 0.020*\"vape\" + 0.015*\"novape\" + 0.012*\"smoke\" + 0.010*\"nosmoke\" + 0.007*\"dalam\" + 0.007*\"school\" + 0.007*\"work\" + 0.006*\"day\" + 0.006*\"today\" + 0.006*\"health\"\n",
      "2022-02-20 13:34:47,864 : INFO : topic #5 (0.083): 0.019*\"novape\" + 0.009*\"nosmoke\" + 0.009*\"day\" + 0.008*\"stopvaping\" + 0.008*\"time\" + 0.007*\"vape\" + 0.007*\"stopvape\" + 0.007*\"stopsmoke\" + 0.006*\"cigarette\" + 0.006*\"good\"\n",
      "2022-02-20 13:34:47,865 : INFO : topic #0 (0.083): 0.062*\"vape\" + 0.020*\"smoke\" + 0.019*\"novape\" + 0.018*\"quit\" + 0.010*\"health\" + 0.009*\"nosmoke\" + 0.009*\"stop\" + 0.008*\"smoking\" + 0.008*\"get\" + 0.008*\"cigarette\"\n",
      "2022-02-20 13:34:47,866 : INFO : topic #4 (0.083): 0.035*\"nicotine\" + 0.025*\"cigarette\" + 0.024*\"vape\" + 0.013*\"juul\" + 0.013*\"teen\" + 0.010*\"know\" + 0.010*\"stopvape\" + 0.009*\"smoke\" + 0.008*\"tobacco\" + 0.007*\"stopvaping\"\n",
      "2022-02-20 13:34:47,867 : INFO : topic #7 (0.083): 0.023*\"stopvaping\" + 0.017*\"stopvape\" + 0.013*\"pada\" + 0.012*\"oml\" + 0.012*\"dm_lingle\" + 0.012*\"soon_toosoon\" + 0.011*\"vape\" + 0.011*\"repost\" + 0.008*\"dengan\" + 0.007*\"meme\"\n",
      "2022-02-20 13:34:47,868 : INFO : topic diff=0.332145, rho=0.577350\n",
      "2022-02-20 13:34:47,869 : INFO : PROGRESS: pass 0, at document #8000/8887\n",
      "2022-02-20 13:34:48,255 : INFO : merging changes from 2000 documents into a model of 8887 documents\n",
      "2022-02-20 13:34:48,266 : INFO : topic #0 (0.083): 0.062*\"vape\" + 0.021*\"quit\" + 0.020*\"smoke\" + 0.014*\"dontvape\" + 0.011*\"health\" + 0.011*\"smoking\" + 0.011*\"novape\" + 0.009*\"cigarette\" + 0.009*\"get\" + 0.008*\"tobacco\"\n",
      "2022-02-20 13:34:48,267 : INFO : topic #7 (0.083): 0.015*\"stopvaping\" + 0.014*\"meme\" + 0.012*\"stopvape\" + 0.012*\"fuck\" + 0.009*\"repost\" + 0.009*\"pada\" + 0.009*\"vape\" + 0.008*\"funny\" + 0.007*\"lol\" + 0.007*\"oml\"\n",
      "2022-02-20 13:34:48,268 : INFO : topic #3 (0.083): 0.038*\"vape\" + 0.022*\"scary_antivape\" + 0.022*\"memes_tidepod\" + 0.013*\"driving_spooky\" + 0.013*\"dontvape\" + 0.012*\"nice\" + 0.011*\"day\" + 0.010*\"love\" + 0.010*\"ita\" + 0.009*\"make\"\n",
      "2022-02-20 13:34:48,269 : INFO : topic #2 (0.083): 0.017*\"vape\" + 0.012*\"dontvape\" + 0.011*\"novape\" + 0.010*\"smoke\" + 0.010*\"smoking\" + 0.009*\"da\" + 0.007*\"pipe\" + 0.007*\"ditchjuul\" + 0.007*\"nosmoke\" + 0.006*\"work\"\n",
      "2022-02-20 13:34:48,270 : INFO : topic #5 (0.083): 0.012*\"novape\" + 0.011*\"dontvape\" + 0.010*\"time\" + 0.009*\"day\" + 0.008*\"quitsmoke\" + 0.007*\"quitnicotine\" + 0.007*\"good\" + 0.007*\"get\" + 0.006*\"vape\" + 0.006*\"stopvaping\"\n",
      "2022-02-20 13:34:48,271 : INFO : topic diff=0.230156, rho=0.500000\n",
      "2022-02-20 13:34:48,593 : INFO : -10.309 per-word bound, 1268.2 perplexity estimate based on a held-out corpus of 887 documents with 23796 words\n",
      "2022-02-20 13:34:48,594 : INFO : PROGRESS: pass 0, at document #8887/8887\n",
      "2022-02-20 13:34:48,829 : INFO : merging changes from 887 documents into a model of 8887 documents\n",
      "2022-02-20 13:34:48,840 : INFO : topic #11 (0.083): 0.032*\"meme\" + 0.023*\"funny\" + 0.015*\"day\" + 0.015*\"follow\" + 0.012*\"novape\" + 0.012*\"dankmeme\" + 0.010*\"funnymeme\" + 0.010*\"dank\" + 0.009*\"make\" + 0.008*\"drag\"\n",
      "2022-02-20 13:34:48,841 : INFO : topic #0 (0.083): 0.057*\"vape\" + 0.029*\"quit\" + 0.019*\"smoke\" + 0.013*\"smoking\" + 0.013*\"health\" + 0.009*\"quitsmoke\" + 0.009*\"get\" + 0.009*\"tobacco\" + 0.009*\"nicotine\" + 0.009*\"help\"\n",
      "2022-02-20 13:34:48,842 : INFO : topic #7 (0.083): 0.017*\"meme\" + 0.012*\"lmao\" + 0.011*\"fuck\" + 0.010*\"stopvaping\" + 0.010*\"ass\" + 0.008*\"repost\" + 0.008*\"stopvape\" + 0.007*\"draw\" + 0.007*\"funny\" + 0.007*\"furry_autistic\"\n",
      "2022-02-20 13:34:48,843 : INFO : topic #5 (0.083): 0.017*\"quitsmoke\" + 0.017*\"easyway\" + 0.016*\"quitnicotine\" + 0.016*\"day\" + 0.014*\"time\" + 0.011*\"photo\" + 0.008*\"good\" + 0.007*\"get\" + 0.007*\"quitsmoking\" + 0.006*\"gum\"\n",
      "2022-02-20 13:34:48,844 : INFO : topic #8 (0.083): 0.038*\"dontvape\" + 0.017*\"programme\" + 0.011*\"dontvapekid\" + 0.011*\"mood\" + 0.010*\"blackandwhite\" + 0.008*\"photography\" + 0.008*\"safety\" + 0.008*\"winter\" + 0.007*\"planet\" + 0.006*\"mountain\"\n",
      "2022-02-20 13:34:48,846 : INFO : topic diff=0.316272, rho=0.447214\n",
      "2022-02-20 13:34:48,847 : INFO : LdaModel lifecycle event {'msg': 'trained LdaModel(num_terms=21122, num_topics=12, decay=0.5, chunksize=2000) in 2.74s', 'datetime': '2022-02-20T13:34:48.847073', 'gensim': '4.1.2', 'python': '3.8.11 (default, Aug  6 2021, 08:56:27) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.14.6-x86_64-i386-64bit', 'event': 'created'}\n",
      "2022-02-20 13:34:48,854 : INFO : LdaState lifecycle event {'fname_or_handle': '/Users/carol/opt/anaconda3/lib/python3.8/site-packages/gensim/test/test_data/model_12.state', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2022-02-20T13:34:48.854561', 'gensim': '4.1.2', 'python': '3.8.11 (default, Aug  6 2021, 08:56:27) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.14.6-x86_64-i386-64bit', 'event': 'saving'}\n",
      "2022-02-20 13:34:48,858 : INFO : saved /Users/carol/opt/anaconda3/lib/python3.8/site-packages/gensim/test/test_data/model_12.state\n",
      "2022-02-20 13:34:48,869 : INFO : LdaModel lifecycle event {'fname_or_handle': '/Users/carol/opt/anaconda3/lib/python3.8/site-packages/gensim/test/test_data/model_12', 'separately': \"['expElogbeta', 'sstats']\", 'sep_limit': 10485760, 'ignore': ['state', 'dispatcher', 'id2word'], 'datetime': '2022-02-20T13:34:48.869492', 'gensim': '4.1.2', 'python': '3.8.11 (default, Aug  6 2021, 08:56:27) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.14.6-x86_64-i386-64bit', 'event': 'saving'}\n",
      "2022-02-20 13:34:48,870 : INFO : storing np array 'expElogbeta' to /Users/carol/opt/anaconda3/lib/python3.8/site-packages/gensim/test/test_data/model_12.expElogbeta.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-20 13:34:48,872 : INFO : not storing attribute state\n",
      "2022-02-20 13:34:48,873 : INFO : not storing attribute dispatcher\n",
      "2022-02-20 13:34:48,873 : INFO : not storing attribute id2word\n",
      "2022-02-20 13:34:48,875 : INFO : saved /Users/carol/opt/anaconda3/lib/python3.8/site-packages/gensim/test/test_data/model_12\n",
      "2022-02-20 13:34:48,882 : INFO : using ParallelWordOccurrenceAccumulator(processes=7, batch_size=64) to estimate probabilities from sliding windows\n",
      "2022-02-20 13:34:53,827 : INFO : 7 accumulators retrieved from output queue\n",
      "2022-02-20 13:34:53,865 : INFO : accumulated word occurrence stats for 9661 virtual documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.43281434037608957, 0.48929086373185193, 0.510331952313632, 0.49521693234122993, 0.4990075088100447]\n",
      "2\n",
      "\u001b[32m******Finished building LDA model******\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carol/opt/anaconda3/lib/python3.8/site-packages/pyLDAvis/_prepare.py:246: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  default_term_info = default_term_info.sort_values(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\n",
      "******Finding the dominant topic in each sentence******\n",
      "\n",
      "\u001b[34m\n",
      "******Find the most representative document for each topic******\n",
      "\n",
      "\u001b[34m\n",
      "******Finding the dominant topic in each sentence******\n",
      "\n",
      "\u001b[34m\n",
      "******Find the most representative document for each topic******\n",
      "\n",
      "\u001b[34m\n",
      "******Finding the dominant topic in each sentence******\n",
      "\n",
      "\u001b[34m\n",
      "******Find the most representative document for each topic******\n",
      "\n",
      "\u001b[34m\n",
      "******Finding the dominant topic in each sentence******\n",
      "\n",
      "\u001b[34m\n",
      "******Find the most representative document for each topic******\n",
      "\n",
      "\u001b[34m\n",
      "******Finding the dominant topic in each sentence******\n",
      "\n",
      "\u001b[34m\n",
      "******Find the most representative document for each topic******\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carol/opt/anaconda3/lib/python3.8/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/Users/carol/opt/anaconda3/lib/python3.8/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/Users/carol/opt/anaconda3/lib/python3.8/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/Users/carol/opt/anaconda3/lib/python3.8/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/Users/carol/opt/anaconda3/lib/python3.8/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/Users/carol/opt/anaconda3/lib/python3.8/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/Users/carol/opt/anaconda3/lib/python3.8/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/Users/carol/opt/anaconda3/lib/python3.8/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "import ujson as uj\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from colorama import Fore\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.test.utils import datapath\n",
    "import gensim.models  \n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "#from gensim.models.wrappers import LdaMallet\n",
    "\n",
    "import spacy\n",
    "import pickle\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "os.environ['MALLET_HOME'] = 'C:\\\\mallet-2.0.8\\\\' \n",
    "\n",
    "class LDA:\n",
    "    # NLTK stop words\n",
    "    stop_words = stopwords.words('english')\n",
    "    stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "    df = None\n",
    "    data = None\n",
    "    data_words = None\n",
    "    bigram = None\n",
    "    trigram = None\n",
    "    bigram_mod = None\n",
    "    trigram_mod = None\n",
    "    fine_data = None\n",
    "    id2word = None\n",
    "    corpus = None\n",
    "    lda_model = None\n",
    "    doc_lda = None\n",
    "    input_file_name = None\n",
    "    best_model = None\n",
    "\n",
    "    def __init__(self, input_file_name):\n",
    "        self.input_file_name = input_file_name\n",
    "        self.load_data()\n",
    "        self.create_bigram_and_trigram()\n",
    "        self.fine_preprocess()\n",
    "        self.create_dict_and_corpus()\n",
    "#         self.build_lda_model()\n",
    "#         for i in range(len(self.model_list)):\n",
    "#             self.format_topics_sentences(ldamodel = self.model_list[i], corpus = self.corpus, texts = self.data, topic_n=i+1)\n",
    "#         #self.format_topics_sentences(ldamodel = self.best_model, corpus = self.corpus, texts = self.data)\n",
    "        \n",
    "    # Load Data\n",
    "    \n",
    "    def load_data(self):\n",
    "        print(Fore.BLUE + \"************LOADING DATA************\")\n",
    "\n",
    "        self.data = []\n",
    "        \n",
    "        input_file = pd.read_csv(self.input_file_name, encoding='unicode_escape')['caption'].dropna().to_numpy()\n",
    "\n",
    "        for line in input_file:\n",
    "#             self.data.append(line[1])\n",
    "            if line is not None:\n",
    "                self.data.append(line)\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "        #with open(self.input_file_name, 'r', encoding='unicode_escape') as input_file:\n",
    "        #    for line in input_file:\n",
    "        #        try:\n",
    "        #            self.data.append(line = line.split(',')[5])\n",
    "        #        except:\n",
    "        #            continue\n",
    "                    \n",
    "        # self.data = self.df.content.values.tolist()\n",
    "        self.remove_noise()\n",
    "\n",
    "        # Tokenize data\n",
    "        self.data_words = list(self.sent_to_words(self.data))\n",
    "#         del self.data\n",
    "\n",
    "        print(Fore.GREEN + \"\\n********LOADING DATA COMPLETE*******\")\n",
    "\n",
    "#     def load_data(self):\n",
    "#         print(Fore.BLUE + \"************LOADING DATA************\")\n",
    "\n",
    "#         self.data = []\n",
    "#         # with open(self.input_file_name, 'r') as input_file:\n",
    "#             # for line in input_file:\n",
    "#                 # self.data.append(uj.loads(line)['text'])\n",
    "\n",
    "#         f = open(self.input_file_name, encoding='UTF8')\n",
    "#         csv_reader = csv.reader(f)\n",
    "#         for line in csv_reader:\n",
    "#             self.data.append(line[1])\n",
    "            \n",
    "#         # self.data = self.df.content.values.tolist()\n",
    "#         self.remove_noise()\n",
    "\n",
    "#         # Tokenize data\n",
    "#         self.data_words = list(self.sent_to_words(self.data))\n",
    "#         # del self.data\n",
    "\n",
    "#         print(Fore.GREEN + \"\\n********LOADING DATA COMPLETE*******\")\n",
    "\n",
    "    # Remove emails, newline, extra spaces, distracting single quote and urls\n",
    "    def remove_noise(self):\n",
    "        # Remove Emails\n",
    "        self.data = [re.sub(r'\\S*@\\S*\\s?', '', sent) for sent in self.data]\n",
    "\n",
    "        # Remove new line characters\n",
    "        self.data = [re.sub(r'\\s+', ' ', sent) for sent in self.data]\n",
    "\n",
    "        # Remove distracting single quotes\n",
    "        self.data = [re.sub(\"\\'\", \"\", sent) for sent in self.data]\n",
    "\n",
    "        # Remove url\n",
    "        self.data = [re.sub(r\"http\\S+\", \"\", sent) for sent in self.data]\n",
    "\n",
    "    # Tokenize data\n",
    "    def sent_to_words(self, sentences):\n",
    "        for sentence in sentences:\n",
    "            yield (gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "    # Create bigram and trigram model\n",
    "    def create_bigram_and_trigram(self, min_count=5, threshold=100):\n",
    "        print(Fore.BLUE + \"\\n******Creating bigram and trigram******\")\n",
    "        self.bigram = gensim.models.Phrases(self.data_words, min_count=min_count, threshold=threshold)\n",
    "        self.trigram = gensim.models.Phrases(self.bigram[self.data_words], threshold=threshold)\n",
    "\n",
    "        self.bigram_mod = gensim.models.phrases.Phraser(self.bigram)\n",
    "        self.trigram_mod = gensim.models.phrases.Phraser(self.trigram)\n",
    "\n",
    "        del self.bigram\n",
    "        del self.trigram\n",
    "\n",
    "        print(Fore.GREEN + \"\\n******Finished creating bigram and trigram******\\n\")\n",
    "\n",
    "    # Remove stopwords, using simple_preprocess in gensim, with stopwords from NLTK\n",
    "    def remove_stopwords(self, texts):\n",
    "        return [[word for word in simple_preprocess(str(doc)) if word not in self.stop_words] for doc in texts]\n",
    "\n",
    "    # Make bigram for the entire dataset\n",
    "    def make_bigram(self, texts):\n",
    "        return [self.bigram_mod[doc] for doc in texts]\n",
    "\n",
    "    # Make trigram for the entire dataset\n",
    "    def make_trigram(self, texts):\n",
    "        return [self.trigram_mod[self.bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "    # Lemmatize dataset, using spaCy\n",
    "    def lemmatization(self, texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "        ret_text = []\n",
    "        for sent in texts:\n",
    "            temp = nlp(\" \".join(sent))\n",
    "            ret_text.append([token.lemma_ for token in temp if token.pos_ in allowed_postags])\n",
    "\n",
    "        return ret_text\n",
    "\n",
    "    # Remove stopwords, bigram, trigram and lemmatization\n",
    "    def fine_preprocess(self):\n",
    "        print(Fore.BLUE + \"******Start fine preprocess******\")\n",
    "        self.fine_data = self.remove_stopwords(self.data_words)  # Remove stop words\n",
    "        self.fine_data = self.make_bigram(self.fine_data)  # Form bigrams\n",
    "        self.fine_data = self.lemmatization(self.fine_data, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "        print(Fore.GREEN + \"******Finished fine preprocess******\\n\")\n",
    "\n",
    "    # Create dictionary and corpus as LDA input\n",
    "    def create_dict_and_corpus(self):\n",
    "        print(Fore.BLUE + \"******Start creating dictionary and corpus for LDA******\")\n",
    "        # Create dictionary\n",
    "        self.id2word = corpora.Dictionary(self.fine_data)\n",
    "\n",
    "        # Create corpus and term document frequency\n",
    "        self.corpus = [self.id2word.doc2bow(text) for text in self.fine_data]\n",
    "\n",
    "        print(Fore.GREEN + \"******Finished creating dictionary and corpus for LDA******\\n\")\n",
    "\n",
    "    # Build LDA model, print perplexity and coherence score. Currently using Mallet\n",
    "    def build_lda_model(self):\n",
    "        print(Fore.BLUE + \"******Start building LDA model******\\n\")\n",
    "        coherence_values = []\n",
    "        model_list = []\n",
    "        mallet_path = '/Users/carol/mallet-2.0.8/bin/mallet' \n",
    "\n",
    "        for num_topics in range(8, 13, 1):  # Here we can specify number of topics desired for the model\n",
    "            print(\"Number of topic is: \", num_topics)\n",
    "#             self.lda_model = gensim.models.wrappers.LdaMallet(mallet_path=mallet_path,\n",
    "#                                                               corpus=self.corpus,\n",
    "#                                                               id2word=self.id2word,\n",
    "#                                                               num_topics=num_topics)\n",
    "            self.lda_model = gensim.models.ldamodel.LdaModel(self.corpus, id2word = self.id2word, num_topics = num_topics)\n",
    "            with open(\"LDA_result_ecig_\" + str(num_topics) + \".csv\", 'w') as output_file:\n",
    "                for i in range(0, self.lda_model.num_topics - 1):\n",
    "                    output_file.write(self.lda_model.print_topic(i) + '\\n')\n",
    "\n",
    "            self.lda_model.save(datapath('model_' + str(num_topics)))\n",
    "\n",
    "            model_list.append(self.lda_model)\n",
    "            coherencemodel = CoherenceModel(model=self.lda_model, texts=self.fine_data, dictionary=self.id2word, coherence='c_v')\n",
    "            coherence_values.append(coherencemodel.get_coherence())\n",
    "            \n",
    "        print(coherence_values)\n",
    "        print(np.argmax(coherence_values))\n",
    "        print(Fore.GREEN + \"******Finished building LDA model******\")\n",
    "\n",
    "        # get best model\n",
    "        self.best_model = model_list[np.argmax(coherence_values)]\n",
    "        # self.best_model.save(\"lda_model.model\")\n",
    "        self.model_list=model_list\n",
    "        return model_list, coherence_values\n",
    "\n",
    "\n",
    "        \n",
    "        # Show graph\n",
    "        # x = range(5, 25, 5)\n",
    "        # plt.plot(x, coherence_values)\n",
    "        # plt.xlabel(\"Num Topics\")\n",
    "        # plt.ylabel(\"Coherence score\")\n",
    "        # plt.legend((\"coherence_values\"), loc='best')\n",
    "        # plt.savefig(\"figure_after.svg\")\n",
    "\n",
    "    # Get perplexity and coherence score\n",
    "    def perplexity_and_coherence(self):\n",
    "        # Compute Perplexity\n",
    "        # print('\\nPerplexity: ', self.lda_model.log_perplexity(self.corpus))\n",
    "\n",
    "        # Compute Coherence Score\n",
    "        coherence = CoherenceModel(model=self.lda_model, texts=self.fine_data, dictionary=self.id2word, coherence='c_v')\n",
    "        print('\\nCoherence Score: ', coherence.get_coherence())\n",
    "        \n",
    "    def plot_difference_plotly(self, mdiff, title=\"\", annotation=None):\n",
    "        \"\"\"\n",
    "        Plot the difference between models.\n",
    "        Uses plotly as the backend.\n",
    "        \"\"\"\n",
    "        import plotly.graph_objs as go\n",
    "        import plotly.offline as py\n",
    "\n",
    "        annotation_html = None\n",
    "        if annotation is not None:\n",
    "            annotation_html = [\n",
    "                [\n",
    "                    \"+++ {}<br>--- {}\".format(\", \".join(int_tokens), \", \".join(diff_tokens))\n",
    "                    for (int_tokens, diff_tokens) in row\n",
    "                ]\n",
    "                for row in annotation\n",
    "            ]\n",
    "\n",
    "        data = go.Heatmap(z=mdiff, colorscale='RdBu', text=annotation_html)\n",
    "        layout = go.Layout(width=950, height=950, title=title, xaxis=dict(title=\"topic\"), yaxis=dict(title=\"topic\"))\n",
    "        py.iplot(dict(data=[data], layout=layout))\n",
    "\n",
    "\n",
    "    def format_topics_sentences(self, ldamodel, corpus, texts, topic_n):\n",
    "        # Init output\n",
    "        sent_topics_df = pd.DataFrame()\n",
    "        # ----- Finding the dominant topic in each sentence -----\n",
    "        print(Fore.BLUE + \"\\n******Finding the dominant topic in each sentence******\\n\")\n",
    "\n",
    "        # Get main topic in each document\n",
    "        for i, row in enumerate(ldamodel[corpus]):\n",
    "            row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "            # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "            for j, (topic_num, prop_topic) in enumerate(row):\n",
    "                if j == 0:  # => dominant topic\n",
    "                    wp = ldamodel.show_topic(topic_num)\n",
    "                    topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                    sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "                else:\n",
    "                    break\n",
    "        sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "        # Add original text to the end of the output\n",
    "        contents = pd.Series(texts)\n",
    "        sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "\n",
    "\n",
    "        sent_topics_df = sent_topics_df.reset_index()\n",
    "        sent_topics_df.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "        sent_topics_df.to_csv('sent_topics'+str(topic_n)+'.csv')\n",
    "        \n",
    "\n",
    "        #  ----- Find the most representative document for each topic -----\n",
    "        print(Fore.BLUE + \"\\n******Find the most representative document for each topic******\\n\")\n",
    "\n",
    "        # Group top 5 sentences under each topic\n",
    "        df_topic_sents_keywords = sent_topics_df\n",
    "        sent_topics_sorteddf_mallet = pd.DataFrame()\n",
    "\n",
    "        sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "        for i, grp in sent_topics_outdf_grpd:\n",
    "            sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n",
    "                                                     grp.sort_values(['Topic_Perc_Contrib'], ascending=[0]).head(1)], \n",
    "                                                    axis=0)\n",
    "\n",
    "        # Reset Index    \n",
    "        sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        # Format\n",
    "        # sent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Text\"]\n",
    "        sent_topics_sorteddf_mallet.to_csv('sent_topics_sorted' +str(topic_n)+'.csv')\n",
    "\n",
    "\n",
    "        # # ----- Topic distribution across documents -----\n",
    "        # print(Fore.BLUE + \"\\n******Topic distribution across documents******\\n\")\n",
    "\n",
    "        # # Number of Documents for Each Topic\n",
    "        # topic_counts = df_topic_sents_keywords['Dominant_Topic'].value_counts()\n",
    "        # # Percentage of Documents for Each Topic\n",
    "        # topic_contribution = round(topic_counts/topic_counts.sum(), 4)\n",
    "        # # Topic Number and Keywords\n",
    "        # topic_num_keywords = df_topic_sents_keywords[['Dominant_Topic', 'Keywords']]\n",
    "        # # Concatenate Column wise\n",
    "        # df_dominant_topics = pd.concat([topic_num_keywords, topic_counts, topic_contribution], axis=1)\n",
    "        # # Change Column names\n",
    "        # df_dominant_topics.columns = ['Dominant_Topic', 'Topic_Keywords', 'Num_Documents', 'Perc_Documents']\n",
    "        # df_dominant_topics.to_csv(\"dominant_topics.csv\")\n",
    "\n",
    "\n",
    "        df = sent_topics_df\n",
    "        df.Dominant_Topic = df.Dominant_Topic.astype(int)\n",
    "        df[\"count\"] = 1\n",
    "        df = df.groupby(\"Dominant_Topic\").sum().reset_index(\"Dominant_Topic\").drop(columns = [\"Document_No\", \"Topic_Perc_Contrib\"])\n",
    "\n",
    "        df_topic_info = sent_topics_sorteddf_mallet\n",
    "        df = df.merge(df_topic_info, on = \"Dominant_Topic\").drop(columns = [\"Document_No\", \"Topic_Perc_Contrib\"])\n",
    "        df.rename(columns = {\"Dominant_Topic\": \"dominant_topic_id\",\n",
    "                             \"count\": \"num_tweets\",\n",
    "                             \"Keywords\": \"topic_keywords\",\n",
    "                             \"Text\": \"example_tweet\"},\n",
    "                            inplace = True)\n",
    "\n",
    "        df[\"perc_tweets\"] = df.num_tweets/df.num_tweets.sum()\n",
    "        df = df[[\"dominant_topic_id\", \"topic_keywords\", \"num_tweets\", \"perc_tweets\", \"example_tweet\"]]\n",
    "        df = df.sort_values(\"num_tweets\", ascending = False)\n",
    "        df.index = np.arange(len(df))\n",
    "        df.to_csv('final_res'+str(topic_n)+'.csv') \n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #TODO: Change input_file_name here\n",
    "    input_file_name = '8887_result.csv'\n",
    "    if input_file_name != None:\n",
    "        lda = LDA(input_file_name)\n",
    "        #model = gensim.models.ldamodel.LdaModel(corpus, num_topics = 10, id2word = id2word)\n",
    "        model_list, coherence_values = lda.build_lda_model()\n",
    "        index = 0\n",
    "        for model in model_list:\n",
    "#             mdiff, annotation = model.diff(model, distance='jaccard', num_words=50)\n",
    "#             lda.plot_difference_plotly(mdiff, title = \"model_\" + str(index))\n",
    "            index = index + 1\n",
    "            corpus = lda.corpus\n",
    "            id2word = lda.id2word\n",
    "            vis = gensimvis.prepare(model, corpus, id2word)\n",
    "            pyLDAvis.save_html(vis,'model_' +str(index) + '.html')\n",
    "           # format_topics_sentences(model, ldamodel, corpus, texts)\n",
    "#         for i in range(len(model_list)):\n",
    "            lda.format_topics_sentences(ldamodel = model, corpus = lda.corpus, texts = lda.data, topic_n=index)\n",
    "    \n",
    "    else:\n",
    "        print(Fore.RED + \"Please specify input file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "e8dbb82e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/carol/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m************LOADING DATA************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 00:44:32,867 : INFO : collecting all words and their counts\n",
      "2022-02-24 00:44:32,869 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\n",
      "********LOADING DATA COMPLETE*******\n",
      "\u001b[34m\n",
      "******Creating bigram and trigram******\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 00:44:33,375 : INFO : collected 185855 token types (unigram + bigrams) from a corpus of 321520 words and 8887 sentences\n",
      "2022-02-24 00:44:33,376 : INFO : merged Phrases<185855 vocab, min_count=5, threshold=100, max_vocab_size=40000000>\n",
      "2022-02-24 00:44:33,378 : INFO : Phrases lifecycle event {'msg': 'built Phrases<185855 vocab, min_count=5, threshold=100, max_vocab_size=40000000> in 0.51s', 'datetime': '2022-02-24T00:44:33.377326', 'gensim': '4.1.2', 'python': '3.8.11 (default, Aug  6 2021, 08:56:27) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.14.6-x86_64-i386-64bit', 'event': 'created'}\n",
      "2022-02-24 00:44:33,379 : INFO : collecting all words and their counts\n",
      "2022-02-24 00:44:33,379 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2022-02-24 00:44:34,623 : INFO : collected 187949 token types (unigram + bigrams) from a corpus of 293609 words and 8887 sentences\n",
      "2022-02-24 00:44:34,624 : INFO : merged Phrases<187949 vocab, min_count=5, threshold=100, max_vocab_size=40000000>\n",
      "2022-02-24 00:44:34,624 : INFO : Phrases lifecycle event {'msg': 'built Phrases<187949 vocab, min_count=5, threshold=100, max_vocab_size=40000000> in 1.25s', 'datetime': '2022-02-24T00:44:34.624827', 'gensim': '4.1.2', 'python': '3.8.11 (default, Aug  6 2021, 08:56:27) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.14.6-x86_64-i386-64bit', 'event': 'created'}\n",
      "2022-02-24 00:44:34,625 : INFO : exporting phrases from Phrases<185855 vocab, min_count=5, threshold=100, max_vocab_size=40000000>\n",
      "2022-02-24 00:44:34,976 : INFO : FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<1677 phrases, min_count=5, threshold=100> from Phrases<185855 vocab, min_count=5, threshold=100, max_vocab_size=40000000> in 0.35s', 'datetime': '2022-02-24T00:44:34.976448', 'gensim': '4.1.2', 'python': '3.8.11 (default, Aug  6 2021, 08:56:27) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.14.6-x86_64-i386-64bit', 'event': 'created'}\n",
      "2022-02-24 00:44:34,977 : INFO : exporting phrases from Phrases<187949 vocab, min_count=5, threshold=100, max_vocab_size=40000000>\n",
      "2022-02-24 00:44:35,342 : INFO : FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<1468 phrases, min_count=5, threshold=100> from Phrases<187949 vocab, min_count=5, threshold=100, max_vocab_size=40000000> in 0.36s', 'datetime': '2022-02-24T00:44:35.342545', 'gensim': '4.1.2', 'python': '3.8.11 (default, Aug  6 2021, 08:56:27) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.14.6-x86_64-i386-64bit', 'event': 'created'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\n",
      "******Finished creating bigram and trigram******\n",
      "\n",
      "\u001b[34m******Start fine preprocess******\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 00:45:09,961 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m******Finished fine preprocess******\n",
      "\n",
      "\u001b[34m******Start creating dictionary and corpus for LDA******\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 00:45:10,165 : INFO : built Dictionary(21122 unique tokens: ['bcpark', 'nosmoke', 'novape', 'signofthetime', 'welcometo']...) from 8887 documents (total 144357 corpus positions)\n",
      "2022-02-24 00:45:10,166 : INFO : Dictionary lifecycle event {'msg': \"built Dictionary(21122 unique tokens: ['bcpark', 'nosmoke', 'novape', 'signofthetime', 'welcometo']...) from 8887 documents (total 144357 corpus positions)\", 'datetime': '2022-02-24T00:45:10.166386', 'gensim': '4.1.2', 'python': '3.8.11 (default, Aug  6 2021, 08:56:27) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.14.6-x86_64-i386-64bit', 'event': 'created'}\n",
      "2022-02-24 00:45:10,287 : INFO : using symmetric alpha at 0.125\n",
      "2022-02-24 00:45:10,288 : INFO : using symmetric eta at 0.125\n",
      "2022-02-24 00:45:10,291 : INFO : using serial LDA version on this node\n",
      "2022-02-24 00:45:10,307 : INFO : running online (single-pass) LDA training, 8 topics, 1 passes over the supplied corpus of 8887 documents, updating model once every 2000 documents, evaluating perplexity every 8887 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2022-02-24 00:45:10,308 : WARNING : too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      "2022-02-24 00:45:10,310 : INFO : PROGRESS: pass 0, at document #2000/8887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m******Finished creating dictionary and corpus for LDA******\n",
      "\n",
      "\u001b[34m******Start building LDA model******\n",
      "\n",
      "Number of topic is:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 00:45:11,046 : INFO : merging changes from 2000 documents into a model of 8887 documents\n",
      "2022-02-24 00:45:11,062 : INFO : topic #6 (0.125): 0.033*\"vape\" + 0.028*\"stopvaping\" + 0.019*\"novape\" + 0.011*\"oml\" + 0.010*\"dm_lingle\" + 0.009*\"soon_toosoon\" + 0.009*\"nosmoke\" + 0.008*\"get\" + 0.008*\"people\" + 0.008*\"stopvape\"\n",
      "2022-02-24 00:45:11,063 : INFO : topic #2 (0.125): 0.023*\"novape\" + 0.016*\"smoke\" + 0.014*\"cigarette\" + 0.014*\"vape\" + 0.013*\"stopvaping\" + 0.011*\"nosmoke\" + 0.007*\"stopvape\" + 0.007*\"day\" + 0.007*\"health\" + 0.006*\"life\"\n",
      "2022-02-24 00:45:11,064 : INFO : topic #0 (0.125): 0.046*\"vape\" + 0.021*\"stopvape\" + 0.013*\"stop\" + 0.013*\"smoke\" + 0.010*\"stopvaping\" + 0.008*\"stopsmoke\" + 0.008*\"novape\" + 0.006*\"know\" + 0.006*\"nicotine\" + 0.006*\"cigarette\"\n",
      "2022-02-24 00:45:11,065 : INFO : topic #4 (0.125): 0.026*\"novape\" + 0.019*\"vape\" + 0.012*\"smoke\" + 0.011*\"stopvaping\" + 0.008*\"quit\" + 0.008*\"nosmoke\" + 0.007*\"stopvape\" + 0.006*\"stop\" + 0.006*\"get\" + 0.006*\"day\"\n",
      "2022-02-24 00:45:11,066 : INFO : topic #5 (0.125): 0.028*\"vape\" + 0.017*\"stopvape\" + 0.015*\"novape\" + 0.014*\"√∞¬æ√∞\" + 0.012*\"smoke\" + 0.009*\"√∞¬æ\" + 0.009*\"smoking\" + 0.009*\"nosmoke\" + 0.008*\"√∞¬º√∞\" + 0.007*\"stopvaping\"\n",
      "2022-02-24 00:45:11,067 : INFO : topic diff=6.991227, rho=1.000000\n",
      "2022-02-24 00:45:11,068 : INFO : PROGRESS: pass 0, at document #4000/8887\n",
      "2022-02-24 00:45:11,567 : INFO : merging changes from 2000 documents into a model of 8887 documents\n",
      "2022-02-24 00:45:11,574 : INFO : topic #7 (0.125): 0.027*\"vape\" + 0.017*\"novape\" + 0.009*\"nosmoke\" + 0.009*\"stopvape\" + 0.007*\"stopvaping\" + 0.007*\"smoke\" + 0.007*\"make\" + 0.006*\"sign\" + 0.006*\"nicotine\" + 0.005*\"cigarette\"\n",
      "2022-02-24 00:45:11,575 : INFO : topic #2 (0.125): 0.021*\"novape\" + 0.017*\"smoke\" + 0.017*\"cigarette\" + 0.014*\"vape\" + 0.009*\"stopvaping\" + 0.009*\"nosmoke\" + 0.008*\"day\" + 0.008*\"time\" + 0.007*\"health\" + 0.007*\"life\"\n",
      "2022-02-24 00:45:11,576 : INFO : topic #6 (0.125): 0.036*\"vape\" + 0.034*\"stopvaping\" + 0.019*\"novape\" + 0.014*\"oml\" + 0.013*\"dm_lingle\" + 0.012*\"soon_toosoon\" + 0.009*\"get\" + 0.008*\"people\" + 0.008*\"juul\" + 0.007*\"nosmoke\"\n",
      "2022-02-24 00:45:11,578 : INFO : topic #1 (0.125): 0.055*\"vape\" + 0.039*\"novape\" + 0.018*\"nosmoke\" + 0.008*\"stopvape\" + 0.007*\"sign\" + 0.007*\"time\" + 0.006*\"get\" + 0.006*\"smoke\" + 0.005*\"make\" + 0.005*\"stopvaping\"\n",
      "2022-02-24 00:45:11,579 : INFO : topic #5 (0.125): 0.029*\"vape\" + 0.019*\"√∞¬æ√∞\" + 0.014*\"novape\" + 0.014*\"stopvape\" + 0.012*\"√∞¬º√∞\" + 0.012*\"√∞¬æ\" + 0.011*\"√∞¬µn\" + 0.011*\"√∞¬µ√∞\" + 0.011*\"smoke\" + 0.010*\"smoking\"\n",
      "2022-02-24 00:45:11,580 : INFO : topic diff=0.562973, rho=0.707107\n",
      "2022-02-24 00:45:11,581 : INFO : PROGRESS: pass 0, at document #6000/8887\n",
      "2022-02-24 00:45:12,025 : INFO : merging changes from 2000 documents into a model of 8887 documents\n",
      "2022-02-24 00:45:12,033 : INFO : topic #4 (0.125): 0.034*\"antivape\" + 0.033*\"quit\" + 0.023*\"novape\" + 0.013*\"vape\" + 0.012*\"smoke\" + 0.010*\"nosmoke\" + 0.009*\"sekarang\" + 0.008*\"health\" + 0.008*\"stopsmoke\" + 0.007*\"get\"\n",
      "2022-02-24 00:45:12,034 : INFO : topic #3 (0.125): 0.050*\"novape\" + 0.024*\"nosmoke\" + 0.016*\"vape\" + 0.012*\"cigarette\" + 0.009*\"quit\" + 0.008*\"nicotine\" + 0.007*\"smoke\" + 0.006*\"day\" + 0.006*\"smokefree\" + 0.006*\"smoking\"\n",
      "2022-02-24 00:45:12,035 : INFO : topic #7 (0.125): 0.028*\"vape\" + 0.023*\"scary_antivape\" + 0.015*\"nice_live\" + 0.015*\"bigchungus_spook\" + 0.011*\"attention_january\" + 0.011*\"th_queenofengland\" + 0.011*\"memes_tidepod\" + 0.011*\"nicotine\" + 0.008*\"novape\" + 0.008*\"driving_spooky\"\n",
      "2022-02-24 00:45:12,037 : INFO : topic #0 (0.125): 0.054*\"vape\" + 0.014*\"stopvape\" + 0.013*\"stop\" + 0.011*\"nicotine\" + 0.011*\"smoke\" + 0.009*\"cigarette\" + 0.009*\"tobacco\" + 0.007*\"know\" + 0.007*\"get\" + 0.007*\"health\"\n",
      "2022-02-24 00:45:12,039 : INFO : topic #2 (0.125): 0.021*\"scary_antivape\" + 0.021*\"memes_tidepod\" + 0.021*\"driving_spooky\" + 0.014*\"world_carlwheezer\" + 0.014*\"jimmyneutron_nfl\" + 0.014*\"memehouse\" + 0.014*\"ricardo_meandtheboy\" + 0.013*\"smoke\" + 0.013*\"day\" + 0.012*\"novape\"\n",
      "2022-02-24 00:45:12,039 : INFO : topic diff=0.595906, rho=0.577350\n",
      "2022-02-24 00:45:12,040 : INFO : PROGRESS: pass 0, at document #8000/8887\n",
      "2022-02-24 00:45:12,669 : INFO : merging changes from 2000 documents into a model of 8887 documents\n",
      "2022-02-24 00:45:12,677 : INFO : topic #7 (0.125): 0.027*\"vape\" + 0.020*\"scary_antivape\" + 0.015*\"bigchungus_spook\" + 0.015*\"nice_live\" + 0.011*\"attention_january\" + 0.011*\"th_queenofengland\" + 0.009*\"nicotine\" + 0.009*\"memes_tidepod\" + 0.007*\"make\" + 0.007*\"dontvape\"\n",
      "2022-02-24 00:45:12,679 : INFO : topic #1 (0.125): 0.044*\"vape\" + 0.019*\"novape\" + 0.012*\"dontvape\" + 0.011*\"nosmoke\" + 0.010*\"nice\" + 0.008*\"driving_nonutnovember\" + 0.008*\"pumpkinspice_antivape\" + 0.007*\"changedotorg_world\" + 0.007*\"carlwheezer_jimmyneutron\" + 0.007*\"meandtheboys_memehouse\"\n",
      "2022-02-24 00:45:12,680 : INFO : topic #3 (0.125): 0.036*\"novape\" + 0.018*\"nosmoke\" + 0.017*\"vape\" + 0.014*\"dontvape\" + 0.012*\"cigarette\" + 0.009*\"quit\" + 0.008*\"nicotine\" + 0.008*\"quitsmoke\" + 0.007*\"smoking\" + 0.007*\"day\"\n",
      "2022-02-24 00:45:12,681 : INFO : topic #0 (0.125): 0.059*\"vape\" + 0.016*\"nicotine\" + 0.014*\"cigarette\" + 0.014*\"dontvape\" + 0.012*\"smoke\" + 0.011*\"tobacco\" + 0.009*\"stop\" + 0.009*\"teen\" + 0.008*\"health\" + 0.008*\"know\"\n",
      "2022-02-24 00:45:12,684 : INFO : topic #4 (0.125): 0.042*\"quit\" + 0.025*\"antivape\" + 0.016*\"novape\" + 0.012*\"vape\" + 0.011*\"smoke\" + 0.009*\"health\" + 0.008*\"nosmoke\" + 0.008*\"quitsmoke\" + 0.007*\"get\" + 0.006*\"smoking\"\n",
      "2022-02-24 00:45:12,685 : INFO : topic diff=0.574756, rho=0.500000\n",
      "2022-02-24 00:45:13,090 : INFO : -9.107 per-word bound, 551.4 perplexity estimate based on a held-out corpus of 887 documents with 23796 words\n",
      "2022-02-24 00:45:13,090 : INFO : PROGRESS: pass 0, at document #8887/8887\n",
      "2022-02-24 00:45:13,438 : INFO : merging changes from 887 documents into a model of 8887 documents\n",
      "2022-02-24 00:45:13,449 : INFO : topic #6 (0.125): 0.028*\"vape\" + 0.015*\"meme\" + 0.010*\"stopvaping\" + 0.009*\"funny\" + 0.008*\"juul\" + 0.007*\"get\" + 0.007*\"people\" + 0.007*\"follow\" + 0.006*\"fit\" + 0.006*\"illness\"\n",
      "2022-02-24 00:45:13,451 : INFO : topic #5 (0.125): 0.076*\"quitvape\" + 0.031*\"vape\" + 0.023*\"quitsmoking\" + 0.021*\"quit\" + 0.021*\"quitsmoke\" + 0.015*\"help\" + 0.015*\"smoking\" + 0.013*\"health\" + 0.011*\"smoke\" + 0.011*\"nicotine\"\n",
      "2022-02-24 00:45:13,452 : INFO : topic #0 (0.125): 0.062*\"vape\" + 0.022*\"nicotine\" + 0.019*\"cigarette\" + 0.013*\"smoke\" + 0.012*\"tobacco\" + 0.010*\"health\" + 0.009*\"stop\" + 0.008*\"know\" + 0.008*\"teen\" + 0.007*\"people\"\n",
      "2022-02-24 00:45:13,454 : INFO : topic #1 (0.125): 0.038*\"vape\" + 0.013*\"novape\" + 0.008*\"dontvape\" + 0.007*\"nice\" + 0.007*\"nosmoke\" + 0.007*\"get\" + 0.006*\"time\" + 0.005*\"pumpkinspice_antivape\" + 0.005*\"driving_nonutnovember\" + 0.005*\"changedotorg_world\"\n",
      "2022-02-24 00:45:13,455 : INFO : topic #2 (0.125): 0.019*\"day\" + 0.013*\"time\" + 0.011*\"smoke\" + 0.011*\"feel\" + 0.011*\"go\" + 0.009*\"cigarette\" + 0.008*\"scary_antivape\" + 0.008*\"memes_tidepod\" + 0.008*\"driving_spooky\" + 0.008*\"year\"\n",
      "2022-02-24 00:45:13,457 : INFO : topic diff=0.688273, rho=0.447214\n",
      "2022-02-24 00:45:13,458 : INFO : LdaModel lifecycle event {'msg': 'trained LdaModel(num_terms=21122, num_topics=8, decay=0.5, chunksize=2000) in 3.15s', 'datetime': '2022-02-24T00:45:13.458346', 'gensim': '4.1.2', 'python': '3.8.11 (default, Aug  6 2021, 08:56:27) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.14.6-x86_64-i386-64bit', 'event': 'created'}\n",
      "2022-02-24 00:45:13,465 : INFO : LdaState lifecycle event {'fname_or_handle': '/Users/carol/opt/anaconda3/lib/python3.8/site-packages/gensim/test/test_data/model_8.state', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2022-02-24T00:45:13.465002', 'gensim': '4.1.2', 'python': '3.8.11 (default, Aug  6 2021, 08:56:27) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.14.6-x86_64-i386-64bit', 'event': 'saving'}\n",
      "2022-02-24 00:45:13,467 : INFO : saved /Users/carol/opt/anaconda3/lib/python3.8/site-packages/gensim/test/test_data/model_8.state\n",
      "2022-02-24 00:45:13,480 : INFO : LdaModel lifecycle event {'fname_or_handle': '/Users/carol/opt/anaconda3/lib/python3.8/site-packages/gensim/test/test_data/model_8', 'separately': \"['expElogbeta', 'sstats']\", 'sep_limit': 10485760, 'ignore': ['state', 'dispatcher', 'id2word'], 'datetime': '2022-02-24T00:45:13.480565', 'gensim': '4.1.2', 'python': '3.8.11 (default, Aug  6 2021, 08:56:27) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.14.6-x86_64-i386-64bit', 'event': 'saving'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 00:45:13,481 : INFO : storing np array 'expElogbeta' to /Users/carol/opt/anaconda3/lib/python3.8/site-packages/gensim/test/test_data/model_8.expElogbeta.npy\n",
      "2022-02-24 00:45:13,484 : INFO : not storing attribute state\n",
      "2022-02-24 00:45:13,485 : INFO : not storing attribute dispatcher\n",
      "2022-02-24 00:45:13,486 : INFO : not storing attribute id2word\n",
      "2022-02-24 00:45:13,488 : INFO : saved /Users/carol/opt/anaconda3/lib/python3.8/site-packages/gensim/test/test_data/model_8\n",
      "2022-02-24 00:45:13,493 : INFO : using ParallelWordOccurrenceAccumulator(processes=7, batch_size=64) to estimate probabilities from sliding windows\n",
      "2022-02-24 00:45:20,040 : INFO : 7 accumulators retrieved from output queue\n",
      "2022-02-24 00:45:20,074 : INFO : accumulated word occurrence stats for 9494 virtual documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4562569244287523]\n",
      "0\n",
      "\u001b[32m******Finished building LDA model******\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carol/opt/anaconda3/lib/python3.8/site-packages/pyLDAvis/_prepare.py:246: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  default_term_info = default_term_info.sort_values(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\n",
      "******Finding the dominant topic in each sentence******\n",
      "\n",
      "\u001b[34m\n",
      "******Find the most representative document for each topic******\n",
      "\n",
      "\u001b[34m************LOADING DATA************\n",
      "\u001b[32m\n",
      "********LOADING DATA COMPLETE*******\n",
      "\u001b[34m\n",
      "******Finding the dominant topic in each sentence******\n",
      "\n",
      "\u001b[34m\n",
      "******Find the most representative document for each topic******\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "import ujson as uj\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from colorama import Fore\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.test.utils import datapath\n",
    "import gensim.models  \n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "#from gensim.models.wrappers import LdaMallet\n",
    "\n",
    "import spacy\n",
    "import pickle\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "os.environ['MALLET_HOME'] = 'C:\\\\mallet-2.0.8\\\\' \n",
    "\n",
    "class LDA:\n",
    "    # NLTK stop words\n",
    "    stop_words = stopwords.words('english')\n",
    "    stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "    df = None\n",
    "    data = None\n",
    "    data_words = None\n",
    "    bigram = None\n",
    "    trigram = None\n",
    "    bigram_mod = None\n",
    "    trigram_mod = None\n",
    "    fine_data = None\n",
    "    id2word = None\n",
    "    corpus = None\n",
    "    lda_model = None\n",
    "    doc_lda = None\n",
    "    input_file_name = None\n",
    "    best_model = None\n",
    "\n",
    "    def __init__(self, input_file_name):\n",
    "        self.input_file_name = input_file_name\n",
    "        self.load_data()\n",
    "        self.create_bigram_and_trigram()\n",
    "        self.fine_preprocess()\n",
    "        self.create_dict_and_corpus()\n",
    "#         self.build_lda_model()\n",
    "#         for i in range(len(self.model_list)):\n",
    "#             self.format_topics_sentences(ldamodel = self.model_list[i], corpus = self.corpus, texts = self.data, topic_n=i+1)\n",
    "#         #self.format_topics_sentences(ldamodel = self.best_model, corpus = self.corpus, texts = self.data)\n",
    "        \n",
    "    # Load Data\n",
    "    \n",
    "    def load_data(self):\n",
    "        print(Fore.BLUE + \"************LOADING DATA************\")\n",
    "\n",
    "        self.data = []\n",
    "        \n",
    "        input_file = pd.read_csv(self.input_file_name, encoding='unicode_escape')['caption'].dropna().to_numpy()\n",
    "\n",
    "        for line in input_file:\n",
    "#             self.data.append(line[1])\n",
    "            if line is not None:\n",
    "                self.data.append(line)\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "        #with open(self.input_file_name, 'r', encoding='unicode_escape') as input_file:\n",
    "        #    for line in input_file:\n",
    "        #        try:\n",
    "        #            self.data.append(line = line.split(',')[5])\n",
    "        #        except:\n",
    "        #            continue\n",
    "                    \n",
    "        # self.data = self.df.content.values.tolist()\n",
    "        self.remove_noise()\n",
    "\n",
    "        # Tokenize data\n",
    "        self.data_words = list(self.sent_to_words(self.data))\n",
    "#         del self.data\n",
    "\n",
    "        print(Fore.GREEN + \"\\n********LOADING DATA COMPLETE*******\")\n",
    "\n",
    "#     def load_data(self):\n",
    "#         print(Fore.BLUE + \"************LOADING DATA************\")\n",
    "\n",
    "#         self.data = []\n",
    "#         # with open(self.input_file_name, 'r') as input_file:\n",
    "#             # for line in input_file:\n",
    "#                 # self.data.append(uj.loads(line)['text'])\n",
    "\n",
    "#         f = open(self.input_file_name, encoding='UTF8')\n",
    "#         csv_reader = csv.reader(f)\n",
    "#         for line in csv_reader:\n",
    "#             self.data.append(line[1])\n",
    "            \n",
    "#         # self.data = self.df.content.values.tolist()\n",
    "#         self.remove_noise()\n",
    "\n",
    "#         # Tokenize data\n",
    "#         self.data_words = list(self.sent_to_words(self.data))\n",
    "#         # del self.data\n",
    "\n",
    "#         print(Fore.GREEN + \"\\n********LOADING DATA COMPLETE*******\")\n",
    "\n",
    "    # Remove emails, newline, extra spaces, distracting single quote and urls\n",
    "    def remove_noise(self):\n",
    "        # Remove Emails\n",
    "        self.data = [re.sub(r'\\S*@\\S*\\s?', '', sent) for sent in self.data]\n",
    "\n",
    "        # Remove new line characters\n",
    "        self.data = [re.sub(r'\\s+', ' ', sent) for sent in self.data]\n",
    "\n",
    "        # Remove distracting single quotes\n",
    "        self.data = [re.sub(\"\\'\", \"\", sent) for sent in self.data]\n",
    "\n",
    "        # Remove url\n",
    "        self.data = [re.sub(r\"http\\S+\", \"\", sent) for sent in self.data]\n",
    "\n",
    "    # Tokenize data\n",
    "    def sent_to_words(self, sentences):\n",
    "        for sentence in sentences:\n",
    "            yield (gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "    # Create bigram and trigram model\n",
    "    def create_bigram_and_trigram(self, min_count=5, threshold=100):\n",
    "        print(Fore.BLUE + \"\\n******Creating bigram and trigram******\")\n",
    "        self.bigram = gensim.models.Phrases(self.data_words, min_count=min_count, threshold=threshold)\n",
    "        self.trigram = gensim.models.Phrases(self.bigram[self.data_words], threshold=threshold)\n",
    "\n",
    "        self.bigram_mod = gensim.models.phrases.Phraser(self.bigram)\n",
    "        self.trigram_mod = gensim.models.phrases.Phraser(self.trigram)\n",
    "\n",
    "        del self.bigram\n",
    "        del self.trigram\n",
    "\n",
    "        print(Fore.GREEN + \"\\n******Finished creating bigram and trigram******\\n\")\n",
    "\n",
    "    # Remove stopwords, using simple_preprocess in gensim, with stopwords from NLTK\n",
    "    def remove_stopwords(self, texts):\n",
    "        return [[word for word in simple_preprocess(str(doc)) if word not in self.stop_words] for doc in texts]\n",
    "\n",
    "    # Make bigram for the entire dataset\n",
    "    def make_bigram(self, texts):\n",
    "        return [self.bigram_mod[doc] for doc in texts]\n",
    "\n",
    "    # Make trigram for the entire dataset\n",
    "    def make_trigram(self, texts):\n",
    "        return [self.trigram_mod[self.bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "    # Lemmatize dataset, using spaCy\n",
    "    def lemmatization(self, texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "        ret_text = []\n",
    "        for sent in texts:\n",
    "            temp = nlp(\" \".join(sent))\n",
    "            ret_text.append([token.lemma_ for token in temp if token.pos_ in allowed_postags])\n",
    "\n",
    "        return ret_text\n",
    "\n",
    "    # Remove stopwords, bigram, trigram and lemmatization\n",
    "    def fine_preprocess(self):\n",
    "        print(Fore.BLUE + \"******Start fine preprocess******\")\n",
    "        self.fine_data = self.remove_stopwords(self.data_words)  # Remove stop words\n",
    "        self.fine_data = self.make_bigram(self.fine_data)  # Form bigrams\n",
    "        self.fine_data = self.lemmatization(self.fine_data, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "        print(Fore.GREEN + \"******Finished fine preprocess******\\n\")\n",
    "\n",
    "    # Create dictionary and corpus as LDA input\n",
    "    def create_dict_and_corpus(self):\n",
    "        print(Fore.BLUE + \"******Start creating dictionary and corpus for LDA******\")\n",
    "        # Create dictionary\n",
    "        self.id2word = corpora.Dictionary(self.fine_data)\n",
    "\n",
    "        # Create corpus and term document frequency\n",
    "        self.corpus = [self.id2word.doc2bow(text) for text in self.fine_data]\n",
    "\n",
    "        print(Fore.GREEN + \"******Finished creating dictionary and corpus for LDA******\\n\")\n",
    "\n",
    "    # Build LDA model, print perplexity and coherence score. Currently using Mallet\n",
    "    def build_lda_model(self):\n",
    "        print(Fore.BLUE + \"******Start building LDA model******\\n\")\n",
    "        coherence_values = []\n",
    "        model_list = []\n",
    "        mallet_path = '/Users/carol/mallet-2.0.8/bin/mallet' \n",
    "\n",
    "        for num_topics in range(8, 9, 1):  # Here we can specify number of topics desired for the model\n",
    "            print(\"Number of topic is: \", num_topics)\n",
    "#             self.lda_model = gensim.models.wrappers.LdaMallet(mallet_path=mallet_path,\n",
    "#                                                               corpus=self.corpus,\n",
    "#                                                               id2word=self.id2word,\n",
    "#                                                               num_topics=num_topics)\n",
    "            self.lda_model = gensim.models.ldamodel.LdaModel(self.corpus, id2word = self.id2word, num_topics = num_topics)\n",
    "            with open(\"LDA_result_ecig_\" + str(num_topics) + \".csv\", 'w') as output_file:\n",
    "                for i in range(0, self.lda_model.num_topics - 1):\n",
    "                    output_file.write(self.lda_model.print_topic(i) + '\\n')\n",
    "\n",
    "            self.lda_model.save(datapath('model_' + str(num_topics)))\n",
    "\n",
    "            model_list.append(self.lda_model)\n",
    "            coherencemodel = CoherenceModel(model=self.lda_model, texts=self.fine_data, dictionary=self.id2word, coherence='c_v')\n",
    "            coherence_values.append(coherencemodel.get_coherence())\n",
    "            \n",
    "        print(coherence_values)\n",
    "        print(np.argmax(coherence_values))\n",
    "        print(Fore.GREEN + \"******Finished building LDA model******\")\n",
    "\n",
    "        # get best model\n",
    "        self.best_model = model_list[np.argmax(coherence_values)]\n",
    "        # self.best_model.save(\"lda_model.model\")\n",
    "        self.model_list=model_list\n",
    "        return model_list, coherence_values\n",
    "\n",
    "\n",
    "        \n",
    "        # Show graph\n",
    "        # x = range(5, 25, 5)\n",
    "        # plt.plot(x, coherence_values)\n",
    "        # plt.xlabel(\"Num Topics\")\n",
    "        # plt.ylabel(\"Coherence score\")\n",
    "        # plt.legend((\"coherence_values\"), loc='best')\n",
    "        # plt.savefig(\"figure_after.svg\")\n",
    "\n",
    "    # Get perplexity and coherence score\n",
    "    def perplexity_and_coherence(self):\n",
    "        # Compute Perplexity\n",
    "        # print('\\nPerplexity: ', self.lda_model.log_perplexity(self.corpus))\n",
    "\n",
    "        # Compute Coherence Score\n",
    "        coherence = CoherenceModel(model=self.lda_model, texts=self.fine_data, dictionary=self.id2word, coherence='c_v')\n",
    "        print('\\nCoherence Score: ', coherence.get_coherence())\n",
    "        \n",
    "    def plot_difference_plotly(self, mdiff, title=\"\", annotation=None):\n",
    "        \"\"\"\n",
    "        Plot the difference between models.\n",
    "        Uses plotly as the backend.\n",
    "        \"\"\"\n",
    "        import plotly.graph_objs as go\n",
    "        import plotly.offline as py\n",
    "\n",
    "        annotation_html = None\n",
    "        if annotation is not None:\n",
    "            annotation_html = [\n",
    "                [\n",
    "                    \"+++ {}<br>--- {}\".format(\", \".join(int_tokens), \", \".join(diff_tokens))\n",
    "                    for (int_tokens, diff_tokens) in row\n",
    "                ]\n",
    "                for row in annotation\n",
    "            ]\n",
    "\n",
    "        data = go.Heatmap(z=mdiff, colorscale='RdBu', text=annotation_html)\n",
    "        layout = go.Layout(width=950, height=950, title=title, xaxis=dict(title=\"topic\"), yaxis=dict(title=\"topic\"))\n",
    "        py.iplot(dict(data=[data], layout=layout))\n",
    "\n",
    "\n",
    "    def format_topics_sentences(self, ldamodel, corpus, texts, topic_n):\n",
    "        # Init output\n",
    "        sent_topics_df = pd.DataFrame()\n",
    "        # ----- Finding the dominant topic in each sentence -----\n",
    "        print(Fore.BLUE + \"\\n******Finding the dominant topic in each sentence******\\n\")\n",
    "\n",
    "        # Get main topic in each document\n",
    "        for i, row in enumerate(ldamodel[corpus]):\n",
    "            row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "            # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "            for j, (topic_num, prop_topic) in enumerate(row):\n",
    "                if j == 0:  # => dominant topic\n",
    "                    wp = ldamodel.show_topic(topic_num)\n",
    "                    topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                    sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "                else:\n",
    "                    break\n",
    "        sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "        # Add original text to the end of the output\n",
    "        contents = pd.Series(texts)\n",
    "        sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "\n",
    "\n",
    "        sent_topics_df = sent_topics_df.reset_index()\n",
    "        sent_topics_df.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "        sent_topics_df.to_csv('sent_topics'+str(topic_n)+'.csv')\n",
    "        \n",
    "\n",
    "        #  ----- Find the most representative document for each topic -----\n",
    "        print(Fore.BLUE + \"\\n******Find the most representative document for each topic******\\n\")\n",
    "\n",
    "        # Group top 5 sentences under each topic\n",
    "        df_topic_sents_keywords = sent_topics_df\n",
    "        sent_topics_sorteddf_mallet = pd.DataFrame()\n",
    "\n",
    "        sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "        for i, grp in sent_topics_outdf_grpd:\n",
    "            sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n",
    "                                                     grp.sort_values(['Topic_Perc_Contrib'], ascending=[0]).head(1)], \n",
    "                                                    axis=0)\n",
    "\n",
    "        # Reset Index    \n",
    "        sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        # Format\n",
    "        # sent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Text\"]\n",
    "        sent_topics_sorteddf_mallet.to_csv('sent_topics_sorted' +str(topic_n)+'.csv')\n",
    "\n",
    "\n",
    "        # # ----- Topic distribution across documents -----\n",
    "        # print(Fore.BLUE + \"\\n******Topic distribution across documents******\\n\")\n",
    "\n",
    "        # # Number of Documents for Each Topic\n",
    "        # topic_counts = df_topic_sents_keywords['Dominant_Topic'].value_counts()\n",
    "        # # Percentage of Documents for Each Topic\n",
    "        # topic_contribution = round(topic_counts/topic_counts.sum(), 4)\n",
    "        # # Topic Number and Keywords\n",
    "        # topic_num_keywords = df_topic_sents_keywords[['Dominant_Topic', 'Keywords']]\n",
    "        # # Concatenate Column wise\n",
    "        # df_dominant_topics = pd.concat([topic_num_keywords, topic_counts, topic_contribution], axis=1)\n",
    "        # # Change Column names\n",
    "        # df_dominant_topics.columns = ['Dominant_Topic', 'Topic_Keywords', 'Num_Documents', 'Perc_Documents']\n",
    "        # df_dominant_topics.to_csv(\"dominant_topics.csv\")\n",
    "\n",
    "\n",
    "        df = sent_topics_df\n",
    "        df.Dominant_Topic = df.Dominant_Topic.astype(int)\n",
    "        df[\"count\"] = 1\n",
    "        df = df.groupby(\"Dominant_Topic\").sum().reset_index(\"Dominant_Topic\").drop(columns = [\"Document_No\", \"Topic_Perc_Contrib\"])\n",
    "\n",
    "        df_topic_info = sent_topics_sorteddf_mallet\n",
    "        df = df.merge(df_topic_info, on = \"Dominant_Topic\").drop(columns = [\"Document_No\", \"Topic_Perc_Contrib\"])\n",
    "        df.rename(columns = {\"Dominant_Topic\": \"dominant_topic_id\",\n",
    "                             \"count\": \"num_tweets\",\n",
    "                             \"Keywords\": \"topic_keywords\",\n",
    "                             \"Text\": \"example_tweet\"},\n",
    "                            inplace = True)\n",
    "\n",
    "        df[\"perc_tweets\"] = df.num_tweets/df.num_tweets.sum()\n",
    "        df = df[[\"dominant_topic_id\", \"topic_keywords\", \"num_tweets\", \"perc_tweets\", \"example_tweet\"]]\n",
    "        df = df.sort_values(\"num_tweets\", ascending = False)\n",
    "        df.index = np.arange(len(df))\n",
    "        df.to_csv('final_res'+str(topic_n)+'.csv') \n",
    "        \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #TODO: Change input_file_name here\n",
    "    #input_file_name = '8887_result.csv'\n",
    "    input_file_name = '8887_result.csv'\n",
    "    if input_file_name != None:\n",
    "        lda = LDA(input_file_name)\n",
    "        #model = gensim.models.ldamodel.LdaModel(corpus, num_topics = 10, id2word = id2word)\n",
    "        model_list, coherence_values = lda.build_lda_model()\n",
    "        index = 0\n",
    "        for model in model_list:\n",
    "#             mdiff, annotation = model.diff(model, distance='jaccard', num_words=50)\n",
    "#             lda.plot_difference_plotly(mdiff, title = \"model_\" + str(index))\n",
    "            index = index + 1\n",
    "            corpus = lda.corpus\n",
    "            id2word = lda.id2word\n",
    "            vis = gensimvis.prepare(model, corpus, id2word)\n",
    "            pyLDAvis.save_html(vis,'model_' +str(index) + '.html')\n",
    "            lda.format_topics_sentences(ldamodel = model, corpus = lda.corpus, texts = lda.data, topic_n=index)\n",
    "    else:\n",
    "        print(Fore.RED + \"Please specify input file\")\n",
    "        \n",
    "    \n",
    "    lda.input_file_name='/Users/carol/Desktop/Ins_anti_combined2200_fv_2.csv'\n",
    "    lda.load_data()\n",
    "    lda.format_topics_sentences(ldamodel = model, corpus = lda.corpus, texts = lda.data, topic_n=index)\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c2595fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m************LOADING DATA************\n",
      "\u001b[32m\n",
      "********LOADING DATA COMPLETE*******\n",
      "\u001b[34m\n",
      "******Finding the dominant topic in each sentence******\n",
      "\n",
      "\u001b[34m\n",
      "******Find the most representative document for each topic******\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carol/opt/anaconda3/lib/python3.8/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/Users/carol/opt/anaconda3/lib/python3.8/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/Users/carol/opt/anaconda3/lib/python3.8/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/Users/carol/opt/anaconda3/lib/python3.8/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/Users/carol/opt/anaconda3/lib/python3.8/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/Users/carol/opt/anaconda3/lib/python3.8/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/Users/carol/opt/anaconda3/lib/python3.8/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/Users/carol/opt/anaconda3/lib/python3.8/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n"
     ]
    }
   ],
   "source": [
    "lda.input_file_name='/Users/carol/Desktop/Ins_anti_combined2200_fv_2.csv'\n",
    "lda.load_data()\n",
    "lda.format_topics_sentences(ldamodel = model, corpus = lda.corpus, texts = lda.data, topic_n=index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25abba77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
